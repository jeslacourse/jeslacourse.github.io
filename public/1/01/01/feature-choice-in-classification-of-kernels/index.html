<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.102.3" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>Feature Choice in Classification of Kernels &middot; jeslacourse</title>
  <meta name="description" content="" />

  
  <link type="text/css" rel="stylesheet" href="https://jeslacourse.github.io/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="https://jeslacourse.github.io/css/poole.css">
  <link type="text/css" rel="stylesheet" href="https://jeslacourse.github.io/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="https://jeslacourse.github.io/css/hyde.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  
</head>

  <body class=" ">
  <aside class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="https://jeslacourse.github.io/"><h1>jeslacourse</h1></a>
      <p class="lead">
       A website built through Hugo and blogdown. 
      </p>
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li><a href="https://jeslacourse.github.io/">Home</a> </li>
        <li><a href="/about/"> About </a></li><li><a href="/experience/"> Experience </a></li><li><a href="/projects/"> Projects </a></li>
      </ul>
    </nav>

    <p>&copy; 2022. All rights reserved. </p>
  </div>
</aside>

    <main class="content container">
    <div class="post">
  <h1>Feature Choice in Classification of Kernels</h1>
  <time datetime=0001-01-01T00:00:00Z class="post-date">Mon, Jan 1, 0001</time>
  <center><b> Abstract</b></center><br>
<p style="text-align:justify"> Frequently, k-nearest neighbors classification is applied with features chosen arbitrarily, while `k` is adjusted to improve the accuracy of the model. For this experiment, `k` is fixed. Using correlations with the strict acknowledgement that all features are continuous, a feature set with high correlation within itself is selection. Visual analysis using density and scatter plots show that these features also share distinct distributions and thusly make great candidates for k-nearest neighbors clustering. The four features return a well classified set, while tweaking the set by removing features with less distinctive groupings returns very high accuracy with an increased risk of overfit. Both features sets perform better than using all features with the same fixed hyperparameter. 
</p>
<h2 id="introduction">Introduction</h2>
<p>With a small database, it&rsquo;s easy enough to fit all features to a model, or select all quantitative data, then frequently modify the hyperparameters to boost up the accuracy. But what information are we losing in the process?</p>
<p>Feature selection is just as important as hyperparameter choice, yet many classifiers cherry-pick features without much thought for their empirical basis. The focus of this experiment will look at the impact of feature selection on wheat kernel classification. Given the distinctive groupings, that occur within many features, k-nearest neighbors will be used for classification. While the hyperparameter, <code>k</code> will be easier to tweak, we must account for the impact of boosting hyperparameters on the risk of overfit.</p>
<h2 id="data-information-and-attributes">Data Information and Attributes</h2>
<h3 id="data-source">Data Source</h3>
<p>Wheat seed data provided by UCI’s Center for Machine Learning and Intelligent Systems.</p>
<center><a href="https://archive.ics.uci.edu/ml/datasets/seeds#" class="uri">https://archive.ics.uci.edu/ml/datasets/seeds#</a></center>
<h3 id="data-information-and-attributes-1">Data Information and Attributes</h3>
<p>There are 210 sample kernels of three varieties of wheat, Kama, Rosa, and Canadian.</p>
<p>Each wheat kernel is assigned seven attributes: grain area, perimeter, compactness, length and width of kernel, asymmetry coefficient and length of kernel groove.</p>
<p>Each measurement is given in millimeters.</p>
<h2 id="data-structure-and-cleaning">Data Structure and Cleaning</h2>
<p>Our source data is a text file with headless, tab-separated data (Table 1).</p>
<center><i>Table 1: Raw Data Input</i></center>
<table style="margin:auto;">
<thead>
<tr>
<th style="text-align:right;">
V1
</th>
<th style="text-align:right;">
V2
</th>
<th style="text-align:right;">
V3
</th>
<th style="text-align:right;">
V4
</th>
<th style="text-align:right;">
V5
</th>
<th style="text-align:right;">
V6
</th>
<th style="text-align:right;">
V7
</th>
<th style="text-align:right;">
V8
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
15.26
</td>
<td style="text-align:right;">
14.84
</td>
<td style="text-align:right;">
0.8710
</td>
<td style="text-align:right;">
5.763
</td>
<td style="text-align:right;">
3.312
</td>
<td style="text-align:right;">
2.221
</td>
<td style="text-align:right;">
5.220
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
14.88
</td>
<td style="text-align:right;">
14.57
</td>
<td style="text-align:right;">
0.8811
</td>
<td style="text-align:right;">
5.554
</td>
<td style="text-align:right;">
3.333
</td>
<td style="text-align:right;">
1.018
</td>
<td style="text-align:right;">
4.956
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
14.29
</td>
<td style="text-align:right;">
14.09
</td>
<td style="text-align:right;">
0.9050
</td>
<td style="text-align:right;">
5.291
</td>
<td style="text-align:right;">
3.337
</td>
<td style="text-align:right;">
2.699
</td>
<td style="text-align:right;">
4.825
</td>
<td style="text-align:right;">
1
</td>
</tr>
</tbody>
</table><br>
<p>To improve readability and analysis, each feature is assigned a descriptive name. The response variable, ‘wheat‘, is factored into a categorical variable with levels &ldquo;Kama&rdquo;, &ldquo;Rosa&rdquo;, and &ldquo;Canadian&rdquo; replacing numeric variables 1,2, and 3, respectively (Table 2).</p>
<center><i>Table 2: Legible Dataframe</i></center>
<table style="margin:auto;">
<thead>
<tr>
<th style="text-align:right;">
area
</th>
<th style="text-align:right;">
perimeter
</th>
<th style="text-align:right;">
compactness
</th>
<th style="text-align:right;">
length
</th>
<th style="text-align:right;">
width
</th>
<th style="text-align:right;">
asymmetry
</th>
<th style="text-align:right;">
groove
</th>
<th style="text-align:left;">
wheat
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
15.26
</td>
<td style="text-align:right;">
14.84
</td>
<td style="text-align:right;">
0.8710
</td>
<td style="text-align:right;">
5.763
</td>
<td style="text-align:right;">
3.312
</td>
<td style="text-align:right;">
2.221
</td>
<td style="text-align:right;">
5.220
</td>
<td style="text-align:left;">
Kama
</td>
</tr>
<tr>
<td style="text-align:right;">
14.88
</td>
<td style="text-align:right;">
14.57
</td>
<td style="text-align:right;">
0.8811
</td>
<td style="text-align:right;">
5.554
</td>
<td style="text-align:right;">
3.333
</td>
<td style="text-align:right;">
1.018
</td>
<td style="text-align:right;">
4.956
</td>
<td style="text-align:left;">
Kama
</td>
</tr>
<tr>
<td style="text-align:right;">
14.29
</td>
<td style="text-align:right;">
14.09
</td>
<td style="text-align:right;">
0.9050
</td>
<td style="text-align:right;">
5.291
</td>
<td style="text-align:right;">
3.337
</td>
<td style="text-align:right;">
2.699
</td>
<td style="text-align:right;">
4.825
</td>
<td style="text-align:left;">
Kama
</td>
</tr>
<tr>
<td style="text-align:right;">
13.84
</td>
<td style="text-align:right;">
13.94
</td>
<td style="text-align:right;">
0.8955
</td>
<td style="text-align:right;">
5.324
</td>
<td style="text-align:right;">
3.379
</td>
<td style="text-align:right;">
2.259
</td>
<td style="text-align:right;">
4.805
</td>
<td style="text-align:left;">
Kama
</td>
</tr>
<tr>
<td style="text-align:right;">
16.14
</td>
<td style="text-align:right;">
14.99
</td>
<td style="text-align:right;">
0.9034
</td>
<td style="text-align:right;">
5.658
</td>
<td style="text-align:right;">
3.562
</td>
<td style="text-align:right;">
1.355
</td>
<td style="text-align:right;">
5.175
</td>
<td style="text-align:left;">
Kama
</td>
</tr>
<tr>
<td style="text-align:right;">
14.38
</td>
<td style="text-align:right;">
14.21
</td>
<td style="text-align:right;">
0.8951
</td>
<td style="text-align:right;">
5.386
</td>
<td style="text-align:right;">
3.312
</td>
<td style="text-align:right;">
2.462
</td>
<td style="text-align:right;">
4.956
</td>
<td style="text-align:left;">
Kama
</td>
</tr>
</tbody>
</table>
<h3 id="missing-values">Missing Values</h3>
<p>Upon initial inspection, the data is complete and contains no missing values.</p>
<p><img src="/assets/images/2020-05-17/figure-markdown_github/unnamed-chunk-3-1.png" alt="">{: .align-center}</p>
<center><i>Figure 1. Outlier and Quantile Analysis by Feature </i></center><br>
<p>Two or three Kama kernel appears to fall outside the normal distribution in several categorical factors (Fig 1), but they are not far enough away from the rest of the level to warrant removal. With no major outliers and no missing values, data cleaning was a relatively simple process involving only the adjustment of
variate names and factor levels.</p>
<h2 id="visualizations-and-analysis">Visualizations and Analysis</h2>
<p>The wheats create distinctive groups when classified by area and perimeter. Length and width also provide significant groupings, while compactness, asymmetry, and groove have substantial overlap between the wheat variants (Fig 2).</p>
<p><img src="/assets/images/2020-05-17/figure-markdown_github/unnamed-chunk-3-2.png" alt=""></p>
<center><i>Figure 2: Histograms with Density Overlays by Feature</i></center><br>
<p>When classifying by wheat groove, in particular, the wheats fall into two distinctive groups, with Canadian and Kama wheats sharing nearly identical distributions.
The groove on Rosa wheat is significantly longer and has almost no overlap with the opposing groups.</p>
<h3 id="correlation-and-associativity">Correlation and Associativity</h3>
<p>With no categorical data, we can create a table of comparative values between all of our variates (Fig 3).</p>
<p><img src="/assets/images/2020-05-17/figure-markdown_github/unnamed-chunk-4-1.png" alt=""></p>
<center><i>Figure 3: Correlation and Associativity </i></center>
<p>Area is highly correlated with the perimeter (r = 0.994), length (r=0.950), and width (r=0.971) of a given kernel. These values are highly correlated with each other as well. Another notable factor, the kernel groove, is also highly correlated with the four noted factors.</p>
<h2 id="classification">Classification</h2>
<p>Given the distinctions in distribution, as well as the high correlation between the given factors, k-nearest neighbors with factors area, perimeter, length, and width as the classifying features. All features are given in millimeters and will not be standardized.</p>
<p>All experiments are completed in <em>R</em> with <code>class::knn</code>, <code>k=5</code>. The train-test ratio will be set at 80/20 to provide an adequate test sample size, <code>n=42</code>. Training set, <code>n=168</code>.</p>
<h3 id="k-nearest-neighbors-with-selected-features">K-Nearest Neighbors with Selected Features</h3>
<center><i>Table 3: Confusion Matrix: Classification with Area, Perimeter, Height and Width</i></center>
<table style="margin:auto;">
<thead>
<tr>
<th style="text-align:left;">
    Pred/Acc 
</th>
<th style="text-align:right;">
Kama
</th>
<th style="text-align:right;">
Rosa
</th>
<th style="text-align:right;">
Canadian
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Kama
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
Rosa
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
Canadian
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
13
</td>
</tr>
</tbody>
</table>
<p>The initial classification did well, 90.47% (Table 3). Two issues stand out: (a) Kama kernels are misclassified relatively frequently, and (b) other kernels are misclassified as Kama kernels relatively frequently. These results are relatively intuitive, as the peak of the distribution for Kama kernels falls between Rosa and Canadian kernels for the factors area, perimeter, length, and width.</p>
<p>The Kama distribution tails for length and width, as noted before overlap significantly. There is very little to no misclassification between Rosa and Canadian wheat kernels as the factor distributions and means are consistently more distant from each other when compared to the difference in means with Kama kernels for either wheat.</p>
<h3 id="k-nearest-neighbors-with-minimal-features">K-Nearest Neighbors with Minimal Features</h3>
<center><i>Table 4: Confusion Matrix: Classification with Area and Perimeter</i></center>
<table style="margin:auto;">
<thead>
<tr>
<th style="text-align:left;">
Pred/Acc    
</th>
<th style="text-align:right;">
Kama
</th>
<th style="text-align:right;">
Rosa
</th>
<th style="text-align:right;">
Canadian
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Kama
</td>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
Rosa
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
Canadian
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
13
</td>
</tr>
</tbody>
</table>
<p>Removing height and width as factors increased the accuracy of our knn prediction to 95.24%, but that number on it’s own raises some suspicion (Table 4). The highly accurate result is likely more accidental, but it does show us that these features with more overlap in the tails do have a significant impact on the number of misclassified values.</p>
<h3 id="k-nearest-neighbors-with-all-features">K-Nearest Neighbors with All Features</h3>
<center><i>Table 5: Confusion Matrix: Classification with All Features</i></center>
<table style="margin:auto;">
<thead>
<tr>
<th style="text-align:left;">
Pred/Acc
</th>
<th style="text-align:right;">
Kama
</th>
<th style="text-align:right;">
Rosa
</th>
<th style="text-align:right;">
Canadian
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Kama
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
Rosa
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
Canadian
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
13
</td>
</tr>
</tbody>
</table>
<p>As a control, we can train with all seven features. Given the significant overlap in compactness, asymmetry, and groove, the accuracy is expected to be lower than when strictly training with area and perimeter. The results are as expected, 90.47% (Table 5).</p>
<h2 id="conclusion">Conclusion</h2>
<p>The experiment shows the value of data exploration when choosing features. The experiment also
shows that increasing the number of features doesn’t necessarily improve the accuracy of testing.
Taking care to minimize inadvertent p-hacking should always be at the forefront of a researcher’s
mind when building experiments.</p>
<p>With that said, the analysis is based off of a single fixed training and test sample. Improvements can
be made to the experiment by replicating the analysis with cross validation, bootstrapping, or k-folds to ensure replicability.</p>

</div>


    </main>

    
      
    
  </body>
</html>
