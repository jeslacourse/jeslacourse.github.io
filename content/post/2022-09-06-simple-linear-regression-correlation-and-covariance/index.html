---
title: Simple Linear Regression - Correlation and Covariance
date: '2022-09-06'
slug: /covariance/
categories: regression
tags: ['correlation', 'covariance' ]
---



<div id="covariance" class="section level2">
<h2>Covariance</h2>
<p>Given the random variables <span class="math inline">\((X,Y)\)</span> <strong>covariance</strong> is defined as:</p>
<p><span class="math display">\[cov(X,Y) = E[(X -\mu_x)(Y-\mu_y)]\]</span></p>
<p>with <strong>sample covariance</strong></p>
<p><span class="math display">\[\hat{cov}(X,Y) = \frac{\Sigma(X_i - \bar{X})(Y_i-\bar{Y})^2}{n-1} \]</span></p>
</div>
<div id="correlation" class="section level2">
<h2>Correlation</h2>
<p><strong>Correlation</strong> is defined on the scale <span class="math inline">\(-1 \leq corr(X,Y) \leq 1\)</span> where:</p>
<p><span class="math display">\[corr(X,Y) = \frac{cov(X,Y)}{\sqrt{var(X)var(Y)}}\]</span></p>
<div id="rules-for-expectation-roll-into-prior-post" class="section level3">
<h3>Rules for Expectation [Roll into prior post]</h3>
<p>Given random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> and constants <span class="math inline">\(a,b,\)</span> &amp; <span class="math inline">\(c\)</span>:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(E(c) = c\)</span></li>
<li><span class="math inline">\(E(cX) = cE(X)\)</span></li>
<li><span class="math inline">\(E(X+Y) = E(X) + E(Y)\)</span></li>
</ol>
<p>An example:
<span class="math display">\[E(a+bX+cY) \\ = a + bE(X) + cE(Y)\]</span></p>
</div>
<div id="rules-for-variance" class="section level3">
<h3>Rules for Variance</h3>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(var(a) = 0\)</span></li>
<li><span class="math inline">\(var(aX) = a^2 var(X)\)</span></li>
<li><span class="math inline">\(var(X+Y) = var(X) + var(Y) + 2cov(X,Y)\)</span></li>
</ol>
<p>An example:</p>
<p><span class="math display">\[var(a+bX-cY) \\= var(a) + var(bX-cY) + 2cov(X,Y)\\= 0 + var(bX)+ var(-cY) \\=b^2var(X) + c^2var(Y)+2[(-bc)cov(X,Y)]\]</span></p>
</div>
<div id="rules-for-covariance" class="section level3">
<h3>Rules for Covariance</h3>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(cov(X,Y) = cov(Y,X)\)</span></li>
<li>if <span class="math inline">\(X ⫫ Y \Rightarrow cov(X,Y) = 0\)</span>, if the random variables are independent of one another, then we cannot predict their variability</li>
<li>if <span class="math inline">\(cov(X,Y)= 0 \Rightarrow\)</span> may or may not <span class="math inline">\(X ⫫ Y\)</span></li>
<li><span class="math inline">\(cov(a,b) = 0\)</span></li>
<li><span class="math inline">\(cov(a, X) = 0\)</span></li>
<li><span class="math inline">\(cov(aX, Y) = a cov(X,Y)\)</span></li>
<li><span class="math inline">\(cov(aX, bY) = ab cov(X,Y)\)</span></li>
<li><span class="math inline">\(cov(X, Y+Z) = cov(X,Y)+cov(X,Z)\)</span></li>
<li><span class="math inline">\(cov(X,X) = var(X)\)</span></li>
</ol>
</div>
</div>
<div id="the-simple-linear-regression-model" class="section level2">
<h2>The Simple Linear Regression Model</h2>
<p><span class="math display">\[Y_i= \beta_0 +\beta_1X_i + \epsilon_i\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(Y_i\)</span>: random variable for experimental unit</li>
<li><span class="math inline">\(X_i\)</span>: predictor for experimental unit, a fixed value</li>
<li><span class="math inline">\(\beta_0, \beta_1\)</span>: parameters, usually known. The goal is to estimate them.</li>
</ul>
<p>The model is considered <strong>simple</strong> because we are using one predictor. The model is linear in parameters and linear in the predictor variable, <span class="math inline">\(X\)</span>.</p>
<p>Properties for <span class="math inline">\(\epsilon_i\)</span>:
1. <span class="math inline">\(E(\epsilon_i) = 0\)</span>, <span class="math inline">\(\forall i\)</span>
2. <span class="math inline">\(var(\epsilon_i)\)</span> is constant
3. <span class="math inline">\(\epsilon_i...\epsilon_n\)</span> are uncorrelated</p>
<p>On average, the error <em>is</em> the regression line, 0. Put another way,</p>
<blockquote>
<p>The expected value of <span class="math inline">\(\epsilon\)</span> is on the regression line.</p>
</blockquote>
<ol style="list-style-type: decimal">
<li></li>
</ol>
<p><span class="math display">\[ E(\epsilon_i) = 0\\
\epsilon_i = Y_i -(\beta_0 +\beta_1X_i) \\
E[\epsilon_i] = E[Y_i -(\beta_0 +\beta_1X_i)] \\
0 = E[Y_i] - E[\beta_0 +\beta_1X_i] \\
0 = E[Y_i] - E[\beta_0] +E[\beta_1X_i] \\
0  = E[Y_i] - \beta_0 + \beta_1X_i \\
   \Rightarrow E[Y_i] = \beta_0 + \beta_1X_i \\
\]</span></p>
<p>showing that on average, <span class="math inline">\(Y_i\)</span> falls along the regression line. We can say the regression lineis the average of <span class="math inline">\(Y_i\)</span>, conditional on <span class="math inline">\(X_i\)</span>.</p>
<p><span class="math display">\[ E(Y_i|X_i)= \beta_0 +\beta_1X_i \]</span>
2. <span class="math inline">\(\epsilon_i...\epsilon_n\)</span> have the same variance</p>
<p>Define $^2(_i) ^2 $, then $^2(Y_i) = var(Y_i) $. <span class="math inline">\(var(\epsilon_i)\)</span> increases as <span class="math inline">\(X\)</span> increases.</p>
<ol start="3" style="list-style-type: decimal">
<li><span class="math inline">\(\epsilon_i...\epsilon_n\)</span> are uncorrelated</li>
</ol>
<p><span class="math display">\[
  corr(\epsilon_i,\epsilon_j) = 0, i\neq j \\
  corr(\epsilon_i,\epsilon_j) = var(\epsilon_i) \neq 0 \\
  cov(\epsilon_i,\epsilon_j) = 0, i\neq j
\]</span></p>
<p>Similarly, but with different notation, <span class="math inline">\(corr(\epsilon_i,\epsilon_j) = \sigma(\epsilon_i,\epsilon_j)\)</span>.</p>
<div id="features-of-models" class="section level3">
<h3>Features of Models</h3>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(Y_i\)</span> is a sum of a constant term plus a random variable. The constant in this case is <span class="math inline">\(\beta_0 +\beta_1X_i\)</span>. The random variable is <span class="math inline">\(\epsilon_i\)</span>.</p></li>
<li><p><span class="math inline">\(E(Y_i|X_i)= \beta_0 +\beta_1X_i\)</span>, see <span class="math inline">\(\epsilon\)</span> property 2.</p></li>
<li><p><span class="math inline">\(\epsilon_i = Y_i -(\beta_0 +\beta_1X_i)\)</span> is a random deviation of <span class="math inline">\(Y_i\)</span> around the regression line.</p></li>
</ol>
<p>It’s worth taking a moment to make the distinction between the regression model, which uses subscripts <span class="math inline">\(i\)</span>:<br />
<span class="math display">\[Y_i= \beta_0 +\beta_1X_i + \epsilon_i\]</span>
and the regression function:</p>
<p><span class="math display">\[Y= \beta_0 +\beta_1X + \epsilon\]</span>
An example:</p>
<p>We want to estimate the systolic blood pressure for a 20 year old. We know the following variables:</p>
<ul>
<li><span class="math inline">\(x\)</span>: age</li>
<li><span class="math inline">\(y\)</span>: systolic blood pressure</li>
<li><span class="math inline">\(\beta_0\)</span>: the intercept, <span class="math inline">\(90\)</span></li>
<li><span class="math inline">\(\beta_1\)</span>: the slope, <span class="math inline">\(0.9\)</span></li>
</ul>
<p>Note that <span class="math inline">\(\beta\)</span>’s are usually unknown. We can say, we expect systolic blood pressure to be <span class="math inline">\(90\)</span> at age <span class="math inline">\(0\)</span> (or at birth), increasing <span class="math inline">\(0.9\)</span> units every year.</p>
<p><span class="math display">\[
E(Y|X = 20) \\= 90 + 0.9(20)\\=108
\]</span></p>
<p>Factoring in an error margin, given <span class="math inline">\(X = 20\)</span> we would expect <span class="math inline">\(Y = 108 +\epsilon\)</span>.</p>
</div>
</div>
<div id="least-squares-estimation" class="section level2">
<h2>Least Squares Estimation</h2>
<div id="estimating-betas" class="section level3">
<h3>Estimating <span class="math inline">\(\beta\)</span>’s</h3>
<p>Find “good” estimates of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> using the <strong>least squares method</strong>.</p>
<p>When you have <span class="math inline">\(n\)</span> pairs of data, <span class="math inline">\((x_1, y_1)...(x_n, y_n)\)</span>:</p>
<p>Use the <strong>deviation</strong> formula</p>
<p><span class="math display">\[e_i = y_i - (\beta_0 +\beta_1x_i)\]</span></p>
<p>to compose the <strong>squared deviation</strong> formula:</p>
<p><span class="math display">\[e_i^2 = [y_i - (\beta_0 +\beta_1x_i)]^2\]</span></p>
<p>This will allow us to define <span class="math inline">\(Q\)</span>, the <strong>sum of squared deviations</strong>:</p>
<p><span class="math display">\[
Q \equiv \Sigma_{i = 1}^n e_i^2\\=\Sigma_{i = 1}^n [y_i - (\beta_0 +\beta_1x_i)]^2
\]</span></p>
<p>The least squares estimates are values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> that minimize <span class="math inline">\(Q\)</span>.</p>
</div>
</div>
