---
title: "Simple Linear Regression"
author: ''
date: "2022-09-05"
draft: no
slug: /regression/
categories: regression
description: ''
---

> One predictor, one outcome

**Linear Regression:** The measure of lineal relationship between two variables. 

Simple regression will have one continuous response variable (y) and one continuous explanatory variable (x). Response variables are also known as the *dependent* or *outcome* variable. Explanatory variables, aka *independent*, *predictor*, or *covariate* variables, can include categorical values. 

$$y=\beta_0 + \beta_1x$$
Where $\beta_0$ is the y-intercept and $\beta_1$ is the slope of the function. 

## Modeling

`X` is treated as a fixed variable whose values have been chosen by the researcher. Though it should be noted that regression is often used where variable is not wholly chosen. For example, sampling a population will return a distribution of ages that may not match the true distribution of the population. So `X` is also, technically, random. 

`Y` is a random variable 

Our regression model dependency of `Y` on `X`
esti
$$Y=\beta_0 + \beta_1X + \epsilon$$
$\beta$'s and `X` are considered fixed values, with random variable $\epsilon$.



Where $\beta_0$ and $\beta_1$ are parameters; usually unknown values related to the population and not the sample. 

- parameter: values for the population
- statistic: values for the sample 

> We are using statistics to estimate our parameters 

After collecting data from the population, we can create a sample on which to run our statistics for which we can estimate our parameters. 

|           | parameter | statistic |
|-----------|-----------|-----------|
| mean      | \mu       | \bar{y}   |
| variance  | \sigma^2  | s^2       |
| slope     | \beta_1   | b_1       |
| intercept | \beta_0   | b_0       |

## Utility

Regression is used for:

1. Observational studies
- just observe, no manipulation
- treatment is not randomized
2. Experimental studies
- manipulate the explanatory variables
- treatment must be randomized

(!) Correlations is not equal to causation in non-random studies 

## Expected Values

For a **discrete** random variable `Y` with possible values $y_1... y_k$ we can say: 
$$ E(Y) = \Sigma_{i=1}^k y_1 P(Y = y_i)$$
where $E(Y)$ is a weighted average of the possible values $y_1... y_k$ and the weights $P(Y = y_1)...P(Y = y_k)$ are the expected probabilities. This will give us a weighted average.

Weighted Average:
$$\Sigma_{i=1}^k w_ia_i$$
If $\Sigma w_i = 1$ and $0 \leq w_1 \leq 1$. 


A **continuous** random variable `Y` would be represented by a density function:

$$E(Y) = \int_{-\infty}^{\infty} yf(y)dy$$
For example, the normal distribution `Y ~ N(0,1)` with mean `0` and variance `1`, would be represented using the density function:

$$E(Y) = \int_{-\infty}^{\infty}y\frac{1}{\sqrt{2\pi}} 
  \exp\left( -\frac{y^2}{2}\right) dy = 0 $$
  
The **population mean** is the expected value. If $Z_1..Z_N$ for all values in the population, then we would use the parameter: 

$$ E(Z) = \frac{1}{N}\Sigma_{i=1}^N Z_i$$

**Sample means** are averages. Given the sample data $X_1... X_n$, would return the *statistic*:

$$ \bar{x}  = \frac{1}{n} \Sigma_{i = 1}^nX_i$$

## Variance

We can use the **population variance** formula to show that $E(Y)$ is equivalent to the parameter $\mu$. Given the population variance $var(Y)$: 


$$ var(Y) = E[Y-E(Y)]^2\\=E[Y-\mu]^2\\\therefore \mu = E(Y)$$

The squaring of $[Y-E(Y)]$ "removes the sign", reverting all distances to positive distances from the mean.  

Given our known substitutions, we can say:

$$ var(Y) = E[Y-E(Y)]^2\\=\Sigma_{i=1}^k (y_i-\mu)^2P(Y-y_i)$$

If $Z_1... Z_n$ account for all values in a population, we would have parameter:

$$var(Z) = \frac{1}{N}\Sigma_{i=1}^{N}(Z_i-\bar{Z})^2$$

where $\bar{Z} = E(Z)$. 


Sample data $x_i...x_n$ would give us the **unbiased sample variance**:

$$s^2 = \frac{\Sigma(x_i - \bar{x})^2}{n-1}$$

Focusing on the denominator, sample sizes $n$ will shift the statistic significantly compared to if the sample was larger. 


## Bias

Because of random variability, $s^2$ will vary with each experiment. An unbiased sample variance is an average value of those experiments. 

$$ E(s^2) = E\frac{\Sigma(Y_i-\bar{Y})^2}{n-1}= var(Y)$$ 