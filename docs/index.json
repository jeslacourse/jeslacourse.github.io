[{"content":"\r\n\r\n\r\n","date":"November 26, 2022","permalink":"/docs/2022-11-26-diagnostic-plots-in-r/","section":"Docs","summary":"","title":"Diagnostic Plots in R"},{"content":"","date":"November 26, 2022","permalink":"/docs/","section":"Docs","summary":"","title":"Docs"},{"content":"","date":"November 26, 2022","permalink":"/","section":"jeslacourse","summary":"","title":"jeslacourse"},{"content":"","date":"November 13, 2022","permalink":"/tags/dataviz/","section":"Tags","summary":"","title":"dataviz"},{"content":"","date":"November 13, 2022","permalink":"/tags/ggplot/","section":"Tags","summary":"","title":"ggplot"},{"content":"\rA Quick Recap\rTable 1: Blood Pressure Sample\rPerson\r1\r2\r3\r4\r5\r6\r7\r8\r9\r10\r11\r12\r13\r14\r15\r16\r17\r18\r19\r20\rAge\r58\r46\r56\r37\r23\r30\r20\r25\r31\r50\r34\r27\r22\r52\r20\r42\r20\r58\r24\r35\rBP\r167\r136\r116\r116\r101\r121\r103\r101\r105\r158\r117\r118\r113\r148\r105\r142\r89\r153\r122\r135\rWe’re looking at the relationship between blood pressure samples and age. For this study, we have 20 people ranging in age between 20 and 58. Our median age is 32.5; mean age is a little higher at 35.5. We also determined that the blood pressure samples are evenly distributed.\nWe found evidence that our linear model was underestimating blood pressure rates for older participants. There was also an individual, Person 3 (56,113), whose blood pressure was substantially lower than expected.\nModel Fit\rThis article is all about the summary read out. Let’s take another look at the summary data for our model:\nsummary(m)\r## ## Call:\r## lm(formula = BP ~ Age, data = data)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -34.248 -6.525 1.099 8.124 15.639 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 76.6333 7.9199 9.676 1.48e-08 ***\r## Age 1.3146 0.2091 6.288 6.28e-06 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 12.36 on 18 degrees of freedom\r## Multiple R-squared: 0.6872, Adjusted R-squared: 0.6698 ## F-statistic: 39.54 on 1 and 18 DF, p-value: 6.284e-06\rR modeling stores a handful of features with the saved variable, these can be accessed by calling names(m). We used the coefficients in the last article, now we’ll take a look at attributes that give us a better look at how well our model fits the data.\nnames(m)\r## [1] \u0026quot;coefficients\u0026quot; \u0026quot;residuals\u0026quot; \u0026quot;effects\u0026quot; \u0026quot;rank\u0026quot; ## [5] \u0026quot;fitted.values\u0026quot; \u0026quot;assign\u0026quot; \u0026quot;qr\u0026quot; \u0026quot;df.residual\u0026quot; ## [9] \u0026quot;xlevels\u0026quot; \u0026quot;call\u0026quot; \u0026quot;terms\u0026quot; \u0026quot;model\u0026quot;\rResiduals\rOur residuals, or differences between the estimated and actual blood pressure measurements, range from an overestimate of 34.2 mmHg, to an underestimate of 15.6 mmHg. The median residual is 1.1mmHg, a slight underestimate. We can determine which residuals belong to a sample by calling resid(m), the equivalent of m$residuals. Our min and max residuals belong to Person 3 and Person 10, respectively.\nTable 2: Blood Pressure Sample\r1\r2\r3\r4\r5\r6\r7\r8\r9\r10\r11\r12\r13\r14\r15\r16\r17\r18\r19\r20\r14.12\r-1.1\r-34.25\r-9.27\r-5.87\r4.93\r0.08\r-8.5\r-12.38\r15.64\r-4.33\r5.87\r7.45\r3.01\r2.08\r10.16\r-13.92\r0.12\r13.82\r12.36\rThese values may be difficult to read on their own, so I’ll add them back to our dataframe and visualize.\ndata$Predicted \u0026lt;- predict(m)\rdata$Residuals \u0026lt;- resid(m)\rWe can now match up the residuals in the table above with the estimates in the figure below. For the figure below, black dots are the actual values, while the black rings along the regression line are the estimated values given the subject’s age. Person 3 and Person 10 are not well estimated, while Person 2, 7, 14, and 18 have a much smaller residual error.\nFigure 1: Residuals: Over- and Underestimates of Blood Pressure\rResidual Standard Error (RSE)\rThe RSE is a standard deviation estimate for linear regression. Smaller values tend to mean a better fit, while an RSE of zero (our estimates perfectly predict our values) would certainly mean our model has been overfit.\n\\[RSE = \\sqrt{SSE/df_R}\\]\rThe residual standard error is the square root value of the sum of squared errors (SSE) divided by the residual degrees of freedom (df). Squaring residuals eliminates the sign (\\(\\pm\\), for an over- or underfit). Taking the mean of residuals otherwise would simply give us a value at or near zero as over- and underestimates cancel each other out. Given 20 samples and 1 parameter (Age), we’ll calculate the degrees of freedom using \\(df_R = n - p - 1\\) or \\(df_R = 20 - 1 - 1 = 18\\).\nSSE \u0026lt;- sum(resid(m)^2) # 2751.189\rdf \u0026lt;- length(m$residuals) - length(m$coefficients) # 18\rRSE \u0026lt;- sqrt(SSE/df) #12.363\rMultiple R-squared\rMultiple R-Squared and the Adjusted R-squared focus on correlation in our model. Correlation can range from -1 to 1, with values closer to -1 or 1 resulting in high correlation. For our data, we have a correlation value of 0.829 (cor(Age,BP)) which gives us a multiple R-squared value of 0.687. Adjusted R-squared values are calculated to account for increased variance with multiple parameters and tend to be more conservative. As such, Adjusted R-squared values are particularly useful for models with several parameters.\ncor(data$Age, data$BP)^2 # 0.687\rF-statistics\rLinear modeling in R uses t-testing and F-statistics to determine if any coefficients significantly influence our dependent variable. In our case, we are testing out whether Age influences BP. In short, a p-value \u0026lt; 0.05 would tell us that Age is a significant coefficient at \\(\\alpha = 0.05\\). In our case, we can see that the p-value for Age is \u0026lt; 0.001; there is a relationship between age and blood pressure.\npvals \u0026lt;- summary(m)$coefficients[,4] # 6.28e-06 In Summary\rThe summary() feature provides critical feedback about the relationship between our dependent variable, BP and our coefficient or independent variable, Age. While not perfect, we have a relatively good model for predicting blood pressure.\nThe next post will look at how to improve our model fit. We’ll use R’s built in diagostics to determine the impact of outliers, sample size, and variance within the data.\n","date":"November 13, 2022","permalink":"/docs/2022-10-06-errors-and-residuals-with-r/","section":"Docs","summary":"Reading \u003ccode\u003elm()\u003c/code\u003e outputs and residual analysis for linear models","title":"Model Fit and Residuals with R"},{"content":"","date":"November 13, 2022","permalink":"/tags/plotly/","section":"Tags","summary":"","title":"plotly"},{"content":"","date":"November 13, 2022","permalink":"/tags/regression/","section":"Tags","summary":"","title":"regression"},{"content":"","date":"November 13, 2022","permalink":"/tags/rstats/","section":"Tags","summary":"","title":"rstats"},{"content":"","date":"November 13, 2022","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":"October 2, 2022","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"October 2, 2022","permalink":"/categories/regression/","section":"Categories","summary":"","title":"regression"},{"content":"\rFitting a Line to Data\rSimple linear regression a method of estimating an outcome based on a single related variable. We’ll want to estimate the systolic blood pressure (our outcome) for 20 subjects based on their age (an independent variable) using R.\nHere are our 20 subjects:\nTable 1: Blood Pressure Sample\rPerson\r1\r2\r3\r4\r5\r6\r7\r8\r9\r10\r11\r12\r13\r14\r15\r16\r17\r18\r19\r20\rAge\r58\r46\r56\r37\r23\r30\r20\r25\r31\r50\r34\r27\r22\r52\r20\r42\r20\r58\r24\r35\rBP\r167\r136\r116\r116\r101\r121\r103\r101\r105\r158\r117\r118\r113\r148\r105\r142\r89\r153\r122\r135\rFirst the Basics\rIt’s good to get a feel for the characteristics of our sample. Our subject information is stored in a dataframe called data.\nWe’ll want to collect some fundamental information about our sample.\nCall summary(data):\n## Person Age BP ## Length:20 Min. :20.00 Min. : 89.0 ## Class :character 1st Qu.:23.75 1st Qu.:105.0 ## Mode :character Median :32.50 Median :117.5 ## Mean :35.50 Mean :123.3 ## 3rd Qu.:47.00 3rd Qu.:137.5 ## Max. :58.00 Max. :167.0\rThe summary tells us we have 20 subjects with a mean age of \\(35.5\\) years. Looking at the quartiles, then our minimum and maximum values, the ages appear to be evenly distributed from 20 to 58 years. A quick check of our blood pressure data shows values that are evenly distributed as well. We’ll look at formal diagnostics later, but right now linear regression is looking like a good tool for estimation.\nVisualizing Data\rThe function ggplotly() from the plotly library is a fantastic way to convert ggplot() items into interactive figures. Note that you can simply use the ggplot() code in the brackets for a static plot.\n{\rggplot(data, aes(name = Person, x = Age, y = BP )) + geom_point() + labs(title = \u0026quot;Blood Pressure by Age, n = 20\u0026quot;, y = \u0026quot;Blood Pressure (mmHg)\u0026quot;, x = \u0026quot;Age (Years)\u0026quot;)\r} %\u0026gt;% ggplotly()\rFigure 1: Blood Pressure by Age, n= 20\rWe certainly have a linear trend. There are slightly more younger subjects than older subjects. Generally speaking, there doesn’t appear to be too much spread (variability) in our data. That said, one of our subjects, Person 3, appears to have a systolic blood pressure substantially lower than expected at for their age; 116 mmHg at the age of 56.\nModeling\rWe appear to be in good shape for modeling with linear regression. Simple linear regression is presented in point-slope form, with parameters \\(\\beta_0\\) and \\(\\beta_1\\) making up our slope and intercept, respectively.\n\\[y = \\beta_0 + \\beta_1x\\]\nIn R, we’ll call lm() for linear modeling. In this case, we want to model blood pressure BP by age.\nlm(formula = BP~Age, data) -\u0026gt; m\r## ## Call:\r## lm(formula = BP ~ Age, data = data)\r## ## Coefficients:\r## (Intercept) Age ## 76.633 1.315\rThe function returns two coefficients, y-intercept (Intercept) and a ‘slope’ for variable, Age. Starting at 76.633 at Age = 0, we can expect a 1.315 mmHg increase in systolic blood pressure with each added year.\nFollowing the regression formula, we’ll set (Intercept) to \\(b_0\\) and Age to \\(b_1\\).\n# Set coefficients to betas\rb0 \u0026lt;- m$coefficients[1] # 76.633\rb1 \u0026lt;- m$coefficients[2] # 1.315\r\\[BP_{est} = b_0 + b_1*Age\\\\\rBP_{est} = 76.663 + 1.315*Age\\]\nWe can now add the regression line to the plot.\n{\rggplot(data, aes(x = Age, y = BP)) +\rgeom_point() + geom_line(aes(y = b0 + b1*Age)) + labs(title = \u0026quot;Estimated Regression Line: Blood Pressure by Age\u0026quot;)\r} %\u0026gt;% ggplotly()\rFigure 2: Regression Line: Blood Pressure by Age\rHovering along the line, we can find the estimated blood pressure for any age in the range.\nFit and Validity\rNote: In this case, (20,103) and (58,153) appear to begin and end the regression line. While it occurs for these data, it’s not common for a regression line to start and end with “true” subject values.\nNotice how the regression line appears to sit in the middle of the points for younger subjects, but after x=40 the line tends to fall under the points. While the regression line appears to be a good fit for the younger subjects, our model frequently underestimates the blood pressure of older subjects.\nSo how good of an estimate is our line? The next post will talk more about residual errors, goodness-of-fit, and diagnostics.\n","date":"October 2, 2022","permalink":"/docs/2022-10-02-simple-regression/","section":"Docs","summary":"An introduction to linear regression with \u003ccode\u003elm()\u003c/code\u003e in R","title":"Simple Regression with R"},{"content":"","date":"May 12, 2020","permalink":"/tags/gghighlight/","section":"Tags","summary":"","title":"gghighlight"},{"content":"\rSide-by-Side Comparison of (a) Generic Plot and (b) Faceted Plots with Highlight\nVisualizing Categorical Data #\rWhat\u0026rsquo;s the most effective way to visualize comparisons between variable factors? How do you share a lot of information without muddying up your visualizations? There definitely isn\u0026rsquo;t one answer, but here\u0026rsquo;s one solution: faceted plots with gghighlight.\nFor sample data, I’m setting up two normal curves with offset sample means x̄ = ± 2. While we\u0026rsquo;re using two factors for simplicity, this method actual becomes more effective when the number of factors increases.1 For our sample data, Lefties are disperses around a mean left of center, while Righties collect right of center. The variances are modified as well to exaggerate the difference in factors, but we\u0026rsquo;ll get into that in a second.\n# create a sample dataset of two normal curves with given classes samples \u0026lt;- data.frame(val = rnorm(100, 2,2), pos = (\u0026#34;Righties\u0026#34;)) %\u0026gt;% rbind(data.frame(val = rnorm(100, -2,1), pos = (\u0026#34;Lefties\u0026#34;)) ) Value Position 1 3.728539 Righties 2 1.465043 Righties 101 -0.210901 Lefties 102 -2.117621 Lefties Statistics #\rSummary statistics as a visual annotation are helpful when determining differences in factors. We have a couple of “out of the box” options through the use of summary and the like.\n# create generic statistical summary samples %\u0026gt;% select_if(is.numeric) %\u0026gt;% map(~tidy(summary(.x))) %\u0026gt;% # compute tidy summary of each var do.call(rbind, .) -\u0026gt; stats # bind list elements into df Minimum Q1 Median Mean Q3 Maximum Value -4.915701 -2.159441 -0.8809261 -0.0567143 1.785081 6.861341 Or we can build our own custom summary table. In this case, we\u0026rsquo;re only interested in adding mean and deviation to our visuals.\n# create a custom summary, in this case, just the mean and sd summary_stats \u0026lt;- samples %\u0026gt;% group_by(pos) %\u0026gt;% summarize_at(vars(val), funs(Mean = mean, SD = sd)) Position Mean SD Righties 1.92 2.22 Lefties -2.03 0.98 Generic Plots #\rGeneric ggplots tend to be a bit bland, which is fine in some cases. The generic plot is almost identical to R\u0026rsquo;s plot() function.2\nGeneric Histogram (w/ Bare Bones Aesthetics)\n# Create a simple histogram (w/ bare bones aesthetics) ggplot(samples, aes(x=val)) + geom_histogram(alpha= 0.7, bins = 40) + # 70% opacity, 40 slices labs(title =\u0026#34;Distribution of Lefties and Righties\u0026#34;, x =\u0026#34;Sample Value\u0026#34;, y= \u0026#34;Frequency\u0026#34;) + # All Labels theme_bw() # B\u0026amp;W Theme In our case, a generic plot shows us that the distribution is bimodal, but we really can\u0026rsquo;t determine a lot about the characteristics of either factors\u0026rsquo; distribution. We are somewhat lucky, in this case, that if we were to create a classifier based strictly off of the information above, we can see that the cutoff should be about x=0. Aside from that, it\u0026rsquo;s difficult to read exactly what\u0026rsquo;s going on.\nFaceted Plots #\rHistograms Faceted by Categorical Factor\nggplot(samples, aes(x=val,fill=pos)) + # Graphs geom_histogram(aes(y=..density..),binwidth=.5, alpha=.5, position=\u0026#34;identity\u0026#34;) + geom_density(alpha=.3) + # One figure per class facet_grid(.~pos, scales = \u0026#34;fixed\u0026#34;)+ # Mean line geom_vline(data = summary_stats, mapping = aes(xintercept = Mean), linetype=\u0026#34;dotted\u0026#34;, color = \u0026#34;blue\u0026#34;, size=0.7) + # Annotations geom_text(data=summary_stats, inherit.aes=FALSE, aes(vjust=\u0026#34;inward\u0026#34;, hjust = \u0026#34;inward\u0026#34;, x = Inf, y = Inf, family = \u0026#34;serif\u0026#34;, label=paste(\u0026#34;\\nMean:\u0026#34;,Mean,\u0026#34;\\nSD:\u0026#34;,SD, \u0026#34;\\n\u0026#34;))) ... Even without the summary statistics, it\u0026rsquo;s much easier to discern the differences in two groups. At this stage, I\u0026rsquo;ve introduced the summary statistics and a mean line to increase comprehension.\nWith that said, there are a handful of important features in this graphic that can be easily overlooked:\nFix the y-axis. Without a fixed axis, we have no ability to quickly discern differences in distribution Drop the opacity of the histogram. Overlapping data can get lost in the shuffle. Adding That Highlight #\rHistograms Faceted by Categorical Factor\nggplot(samples, aes(x=val,fill=pos)) + geom_histogram(aes(y=..density..),binwidth=.5, alpha=.5, position=\u0026#34;identity\u0026#34;) + geom_density(alpha=.3) + \u0026lt;b\u0026gt;gghighlight::gghighlight()\u0026lt;/b\u0026gt; # add gghighlight ... This is such a beautiful, yet deceptively simple trick to improving readability, a gghighlight!3 It\u0026rsquo;s worth noting that we don\u0026rsquo;t need any additional arguments as the function is highlighting the given data per facet. Put another way, gghighlight is intuitive enough to figure out what should be grayed out and what should pop. Highlighting works with more than two factors as well. Anything that isn\u0026rsquo;t the primary data simply sits in the background.\nWe\u0026rsquo;re just scraping the surface of what gghighlight can do for data viz. It\u0026rsquo;s incredibly effective for singling out notable traits in a class, emphasizing trends, or simply comparing a subset of data to the rest of a sample.\nGGPlot Summary #\rThere are a lot of bits and pieces to these graphs, here\u0026rsquo;s a quick recap on what role each piece plays.\ngeom_histogram: creates histogram, plotting feature values against their frequencies\ny=..density.. shifts the y-axis to a relative frequency\nbin_width: the number of “slices” that we cut our data into. Play with this number! Can be relative, 0-1, or a number of slices (i.e. binwidth = 10 for ten equal sized bins)\nalpha: the transparency of our data, from fully transparent (0.0) to opaque (1.0)\nposition: relationship between the classes, can be “identity”, “stack”, “dodge”. Using facet_grid, “identity” has the added benefit of creating equal axes for each figure.\ngeom_density: smooths the histogram into a density plot, with y-axis as the relative frequency for a point\ngeom_vline: vertical line, in this case used to create a mean line\ngghighlight: great way to show our data compared to the relative spread. Creates a “shadow” of the overall distribution.\nfacet_grid: breaks visuals into individual figures by feature.\nThe researcher that inspired this post needed to present characteristics for six separate factors in a concise manner.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI\u0026rsquo;m oversimplifying a bit. This plot isn\u0026rsquo;t completely generic. Titles, transparency, and theming aren\u0026rsquo;t necessarily something you would get with base plot().\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRDocumentation: gghighlight.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"May 12, 2020","permalink":"/docs/2020-05-12-facet-figures/2020-05-12-facet-figures/","section":"Docs","summary":"Using \u003ccode\u003egghighlight()\u003c/code\u003e to effectively visualize comparisons between factors","title":"Highlights and Facets for Visualizing Categorical Data"},{"content":"","date":"May 12, 2020","permalink":"/categories/rstats/","section":"Categories","summary":"","title":"Rstats"},{"content":"","date":"May 12, 2020","permalink":"/categories/statistics/","section":"Categories","summary":"","title":"Statistics"},{"content":"","date":"May 12, 2020","permalink":"/categories/visualizations/","section":"Categories","summary":"","title":"Visualizations"},{"content":" I\u0026rsquo;ll likely be revisiting this project in the future to see how my analysis has changed. With that said, I\u0026rsquo;m presenting this project as I did in 2016. So much has changed in the last four years. I\u0026rsquo;m excited to rework this project and pull in some fresh data. It should make for a very interesting before-and-after.\nFinal product, top left, with the three layers used for analysis and composition\r{% include gallery id=\u0026ldquo;gallery\u0026rdquo; caption=\u0026ldquo;Paper Map Examples at Regional, District and City Levels\u0026rdquo; %}\nGrassroots Organization for Presidental Primary Campaigning • Spring 2016\nWashoe County Canvassing, Feb 2016\nProject Idea and Timeline #\rCreate a canvass map to help direct campaign efforts, with a focus on targeting likely voters in Northern California. Ideally this will be a dynamic project, updated as precincts are contacted.\nPrimary Data #\rAmerican Community Survey 5‐Year Estimates — TIGER/Line1\nStatewide Database – Hosted by University of California, Berkeley 2\nTimeline #\rCollect and sort 2008/2012 elections data and precinct information Find hotspots, Precincts with higher turnouts and moderate to high Democrat preference, to canvass first. Most valuable precincts will have a moderate to high population of registered voters and good election turnouts. Compile map, determine most effective map style Compile statistical analysis Develop an interface to display precincts with canvassed and not canvassed information May be best done through a web app Project Summary #\rThe intent of this project is to determine the best voting precincts, within the state of California, in which to send political canvassers. Priority will be based on favorable 2012 General Election and Registration demographics, as well as 2014 Census demographics. The map will be best utilized by a small team or individual campaign coordinator who will then send teams out to the highest priority precincts.\nPurpose #\rIt’s a given fault that canvassing efforts generally cannot reach all of the neighborhoods that they want to. So how do you prioritize which neighborhoods to visit and which get the inevitable pass?\nI’ve established three criteria:\nHigh density areas are a big target. High density areas, in the context of this project, will be defined in two different ways.\nPrecincts with the highest rate of Democrat and “No Party Preference (NPP)” counts.\nAreas with overall high population densities as well, since voters can register or change their affiliation up until May 23rd, 2016.\nIt’s important to know the difference between high population densities and high voter registration densities. It’s not enough to know how many people can vote. It’s more important, at this stage, to focus on confirmed registered voter numbers. With that said,\nTurnout, the quantity of registered voters that actually voted in past elections, will weigh in on a precinct’s canvass priority. In a hypothetical situation, a high turnout in a lower population precinct may prove more valuable than a moderately low turnout in a higher population area. Giving each value an equal weight will allow us to quantitatively determine where to send our canvassers. Process #\rData Research #\rVoter Turnout and Registered Voter Data\nOur state is broken up into over 21,000 voter precincts, which may also be referred to as election districts. Each precinct divides a city or census designated place to distribute the population among relatively equal populations.\nElection results and voter demographic information can be linked to precinct shapefiles.\n2008 v. 2012\nThe differences in voting demographics between 2008 and 2012 is not substantial, so emphasis is placed on relevance. The more recent information should prove more accurate and influential than any trends that occurred with the higher turnout of 2008.\nPopulation Density\nLikewise, Census information via the TIGER database can be used to collect general population density information. The detailed Census demographic and population information, linked to the land area of the state’s census shapefiles, can provide us with the population density. Creating a population density graphic will be most accurate at the block group level (block level unavailable).\nData Sources #\rStatewide Database – Hosted by University of California, Berkeley2\n2012 State Precincts Shapefile 2012 Voter Turnout Demographics Table (.dbf) 2012 Voter Registration Demographics Table (.dbf) American Community Survey 5-Year Estimates — TIGER/Line1\n2014 California Block Group Shapefile 2014 5‐Year Estimate “X01 Age and Sex” Demographics Table (.dbf) TIGER/Line Shapefile – California Primary and Secondary Roads, State County Shapefiles USGS Water Resources – California Lakes Natural Earth Data – Oceans\nData Management and Joins #\rBefore joining the data, some adjustments had to be made to make the information more palatable for mapping. Joins were made between the Precinct shapefile and both the voter turnout table and registration table. The Census block groups and demographic table were joined based on the block group ID number. The newly joined tables and shapefiles were then exported as three shapefiles into the master geodatabase.\nNull Values\nThe precinct data tables included areas referred to as “unassigned”. For voter turnout this meant that the geographic area had no voters turn out. Likewise for registration, unassigned areas did not have any registered voters.\nTo aid with the conversion from vector to raster, as well as visualization with the vector data itself, several custom tables were created to reduce null value and divide‐by‐zero errors. Null values were converted to zero.\nA pre‐logic script code was then added to several new custom fields eliminate the divide‐by‐zero errors. The custom field details are noted in the next section. This is a snippet of VB Script used to divert values to zero in locations where there are no registered voters. If the number of registered voters is greater than zero, the calculation will run as expected.\n​``` If [TOTREG] = 0 Then val = 0 Else val = [TOTVAL]/[TOTREG] End If ​``` Custom Fields and Field Calculations\nThree custom fields were created for this project:\nTurnout Rate Target Voters Census Density and Demographic Turnout Rate – Calculate the percentage of registered voters that turned out to vote. Divide the number of vote by the number of registered voters accounted for.\n[Turnout] = [TOTREG]/[TOTVOTE]\nTarget Voters – Calculate the percentage of total registered voters that have declared themselves as Democrats or have declined to state a party.\n[TV_Vote] = ([DEM]+[DCL])/[TOTREG_R] Target Vote = (Democrat + Decline to State)/Total Registered Voters Census Density with Target Demographic – a) Remove all children from each block group’s total counts, then, b) divide the adult count by the land area of their respective block group\n[Voting_Age] = [B00001e1] ‐ ([B01001e3]‐[B01001e4]‐[ B01001e5]‐[B01001e27]‐[B01001e28]‐[ B01001e29]) Voting Age = Total Count – (“Male \u003c5” – “Male 5‐9” – “Male 10‐14” – “Female \u003c5” – “Female 5‐9” – “Female 10‐14”)\n[Census_Density] = [Voting_Age]/[ALAND] Block Group Density = Voting Age Adults/Block Group Land Area (sq. meters)\nModifications to Jenks\n(a) Jenks on Target Voters Layer and (b) Turnout Raster All vector maps were symbolized using the Jenks method with four breaks (five sections).\nIn order to adequately show the difference between areas with no and low respective rates, the Jenks scale was modified to show six sections instead of five. The sixth section is simply zero values.\nA section of the target voters map is above, (a). The white stripes over what is effectively Folsom Lake are areas with no registered voters. This contrasts the low Democrat and Decline to State (NPP) rates in the Lake’s surrounding neighborhoods, shown in red. The blue symbology in the northeast corner is an area of substantially higher Democrat/NPP rates.\nPolygon to Raster Conversion #\rUsing the three newly created fields outlined in the Custom Fields and Field Calculations section, each of the three main shapefiles were converted to raster using the Polygon to Raster tool. Below is the conversion for the Turnout_Rate shapefile to raster. This process creates three new rasters:\nTurnout_Rate_Raster, Target_Voter_Raster, and Census_Density_Raster. The new rasters were added to the master geodatabase upon completion. A snippet of the turnout rate near Folsom Lake is shown above, (b). Areas of dark green have the highest turnouts. The scale moves through the light greens to yellows, oranges, then to bright red denoting zero turnout.\nRaster Math #\r(a) The Multiplier and (b) Multiplier Raster with Precinct Poly Overlay Using the raster calculator, the three raster layers are multiplied together to produce a new raster, The_Multiplier. By multiplying the raster layers together, equal weight is given to the turnout, target voters and population densities of a specific area. Precinct boundaries and census block group boundaries are completely disregarded in this step.\nAbove, (a), is Folsom Lake and it’s surrounding areas, primarily to the south and west. As we’ve seen with the vector and raster maps previously, the Lake holds a very low priority value with regard to canvassing. To the south, however, is the northern section of the City of Folsom. City of Folsom shows off it’s highest priority areas, in dark green, to it’s lowest, deep red. Gray areas hold no significant population, near zero election turnouts, or near zero Democrat/NPP numbers and are considered the lowest priority.\nZonal Statistics #\rWith the now blended Multiplier raster created, the information needs to be related back to the precinct shape files. The image, (b), shows the “blended” raster layer with the precinct overlaid. It’s clear that several precincts are not well defined. This next step will resolve that.\nThe Zonal Statistics as Table tool, below, takes the mean of each of the Multiplier raster layer within each precinct boundary. The mean information is then added to a newly created table (.dbf)\nThe new table, Multiplier_Table, is joined to the Precinct Boundary shapefile. The data is symbolized by the mean value the raster cells within each precinct.\nBy using the Multiplier raster and table as an intermediary, we’ve effectively converted our original census and precinct vector data into a dataset that fits neatly within the precinct boundaries.\nPrecincts now prioritized based on voter turnout, registration demographic, and population density Finishing Touches #\rCongressional District 13 (a) without and (b) with Highways, Hydrology and Cities To improve the map’s readability, recognizable features like highways, lakes and major city labels have been added to the map.\nThis map is heavily based on voting precinct boundaries and Congressional District boundaries. These are two features that may not be inherently recognizable to the reader. More familiar boundaries (e.g. county lines, hydrologic features) have been added to improve readability.\nProblems #\rData Sources #\rPrecinct Information\nCalifornia, more specifically the Secretary of State, does not have an aggregated list of general election votes and voter registrations by precinct available to the public. As a result, the 2012 demographic and voter information is sourced through a third party. The Berkeley School of Law information has aggregated datasets from the Statements of Vote (SOV) and Statements of Registration from California’s 58 counties, as collected by each county’s respective County Registrar of Voters or County Clerks.\nCensus Information\nThere is a modest learning curve that comes with the readability of Census data. With some research, there are resources available to decode file names, field names and field descriptions. Census data is also laden with concatenations and acronyms. Fortunately, the Census has made documentation widely available online.\nVariance in Population #\rCongressional District 1, Lowest Density CD Still Has A\rWell Represented City – Chico, CA\rThe initial intent of this project was to create a canvass map for Northern California. Unfortunately, the state has a delegate distribution process that would make it very difficult to focus on the northern part of the state alone.\nWith six delegates assigned to each Congressional District, then 105 delegates at‐large distributed after the primary election, it was increasingly apparent that the population heavy Los Angeles precincts should be included in dataset. The decision was made to prevent any major skewing of the prioritization process. It also had the added benefit of providing the same spatial analysis to what potentially could be the most delegate rich section of the state.\nCongressional District 12 – San Francisco\rLargely Overrepresented\rCreating a scale that allowed for at least one high priority precinct in each Congressional District was essential. The statistical over-representation of more populated areas was considered acceptable, as they will ultimately have the largest influence on at‐large delegates.\nProduction #\rWeb Map, Limited Congressional Districts ‐ http://arcg.is/1TnmgNs Link out of date\nI developed this project to challenge myself. Many aspects of this project were brand new to me and it was an absolute thrill to be able to troubleshoot my way through it.\nThe data research proved more challenging than I expected, but good data is the literal cornerstone of a good project. Utilizing two completely different data sources, finding the shapefiles, inspecting the data tables then ensuring my joins would be adequate, took up the first third of my semester.\nCreating a formula that answered the question, “What gives a precinct a higher priority over another?” proved to be the largest challenge. My answer: a top priority precinct will have a moderate to high population density, high voter turnout in the 2012 General Election, and a high rate of Democrat and unaffiliated voters. These are the best areas to send an individual out into the field to canvass and expect not only a good number of contacts made, but fair majority of positive interactions.\nConverting data from vector to raster is old hat. But how do you convert a custom raster layer to fit the polygons of an established vector file? What would the data quality be like? What I loved about this project was the opportunity to dive right into the spatial analysis tools like the raster calculator and zonal statistics. This project’s two major shapefiles, 2012 Election Precincts and 2014 Census Block Groups, were not compatible with regard to sharing information back‐and‐forth. With the data converted to raster, I had a whole new picture to look at.\nRaster math allowed me to set equal rates for each of my three criteria, zero to one, then give each rate equal weight by multiplying the three rates together. What came next was all new. I have a raster map that no longer fits the shapes I started with. The Zonal Statistics toolset was an absolute treat to work with. Creating the statistics table based on the raster cells, limited to the precinct shapefile boundaries, was the most novel part of this project for me.\nAn unexpected result of this project was the depth in which I was able to work with the raster data. Previous projects made rasters feel heavy and unnecessary. This project allowed me to manipulate data to the extent I needed to, then convert it back to vector data with the intent of using the shapefiles in an online environment. Had I not planned to create a web map, the raster data may have proven robust enough for its own project.\nhttps://www.census.gov/geo/maps‐data/data/tiger‐data.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttp://statewidedatabase.org/d10/index.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"April 18, 2020","permalink":"/docs/2020-04-18-california-canvass-map/2020-04-18-california-canvass-map/","section":"Docs","summary":"I\u0026rsquo;ll likely be revisiting this project in the future to see how my analysis has changed.","title":"California Canvass Map"},{"content":"","date":"April 18, 2020","permalink":"/tags/gis/","section":"Tags","summary":"","title":"GIS"},{"content":"","date":"April 18, 2020","permalink":"/categories/gis/","section":"Categories","summary":"","title":"GIS"},{"content":"","date":"April 18, 2020","permalink":"/tags/spatial-analysis/","section":"Tags","summary":"","title":"spatial analysis"},{"content":"","date":"April 18, 2020","permalink":"/tags/statistics/","section":"Tags","summary":"","title":"statistics"},{"content":"","date":"April 18, 2020","permalink":"/tags/volunteer-work/","section":"Tags","summary":"","title":"volunteer work"},{"content":" The original post is a response to a class forum. This post is just a handy way to determine how the site will handle markdown language while also providing some interesting content.\nThis was originally part of a response to #151, but it should be good information for anyone trying to troubleshoot git version control. I\u0026rsquo;ve pulled some of the specific troubleshooting out and added a bunch of detail on the common commands. My hope is that it gives you guys a kind of understanding of what git is doing in the background.\nPleeease be careful about just blindly typing in git commands. As safe as git version control is, you run a real risk of overwriting your files by just trying to pull simply when git asks you to.\nShort version: #\rgit clone url: Asking the remote (online) repository to make a local (PC) copy\ngit pull: Including or \u0026ldquo;pulling down\u0026rdquo; changes from the remote repository to your local\ngit status: Looking at the differences between the remote and local. Awesome tool for troubleshooting.\ngit diff: Shows changes between the remote file and the local\ngit add: Prep stage getting ready to add new files that are local but you want to be remote. This is also where git starts tracking files if you\u0026rsquo;ve heard that term before.\ngit commit: Takes the files to be updated to the remote repo, creates a \u0026ldquo;timestamp of sorts\u0026rdquo; and stages them to be included in the remote repo, i.e. you\u0026rsquo;re committing the changes you\u0026rsquo;ve made up to a certain point. For us, this is usually just one commit (when we complete the file).\ngit push: Updates the remote repository\nA bit more in-depth #\rGit Pull So let\u0026rsquo;s talk real quick about what git pull does, especially considering if you\u0026rsquo;re pushing your changes you likely have a complete project that shouldn\u0026rsquo;t be altered much. git pull is pulling remote (online) changes down to your local (PC) repository. Disclaimer, I use Git Bash console for my commands, so RStudio will be slightly different, but the commands are the same. For this class, we don\u0026rsquo;t really pull assignments as much since we\u0026rsquo;re simply using git clone url to create a local copy. git pull is more effective in cases like #80 where there\u0026rsquo;s been a revision.[For revisions see : #153] Assuming you\u0026rsquo;ve already cloned the file, git pull will \u0026ldquo;pull down\u0026rdquo; any changes made to the remote repository and include them in your local file.\nGit Status and Diff But you\u0026rsquo;ve been working on your files before the changes were made remotely, right? Now you have a high risk of merge conflicts. Try out git status to see what files have changed.\nMine looks like this:\n$ git status\rOn branch master\rYour branch is up-to-date with \u0026#39;origin/master\u0026#39;. # where your conflict is Changes not staged for commit:\r(use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to update what will be committed)\r(use \u0026#34;git checkout -- \u0026lt;file\u0026gt;...\u0026#34; to discard changes in working directory)\rmodified: q1.Rmd\rmodified: q2.Rmd\rmodified: q3.Rmd\rUntracked files:\r(use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to include in what will be committed)\r.RData\r.Rhistory\rq1.html\rq2.html\rno changes added to commit (use \u0026#34;git add\u0026#34; and/or \u0026#34;git commit -a\u0026#34;) The first few lines are telling me that I\u0026rsquo;m on the main branch (we won\u0026rsquo;t talk about branches much here, so this won\u0026rsquo;t change for the time being) and that my (local) branch, or copy of the repo, is up-to-date with the remote repository, called \u0026lsquo;origin/master\u0026rsquo;.\nAfter that we start to see file status changes. I\u0026rsquo;m currently working on q3.Rmd, and haven\u0026rsquo;t pushed any changes yet, so it\u0026rsquo;s intuitive that my modified files would be the three *.Rmd files.\nIf for some reason status tells me that I'm x commits behind masterI can look at the difference between my repo and the remote repo line-by-line withgit diff`.\n$ git diff diff --git a/q1.Rmd b/q1.Rmd index 5b76ee4..7dfe81a 100644 #commit references\r--- a/q1.Rmd # remote file\r+++ b/q1.Rmd # local file\r@@ -1,5 +1,6 @@ # lines changes\r---\rtitle: \u0026#34;Question 1\u0026#34;\r+author: \u0026#34;Jessica LaCourse\u0026#34;\r---\r@@ -15,32 +16,75 @@ The `walk` function is very similar to the `map` function except that it doesn\u0026#39;t Here, a/q1.Rmd b/q1.Rmd are my remote and local files being compared, respectively. Values between the @ symbols are line references and the + is a line, an author tag at the start of assignment 1, I added to the start of the file. Intuitively, you can also have - changes for lines deleted. git diff is especially handy when you are working in a team and the remote repo is changing while you\u0026rsquo;re working locally.\nGit Add, Commit and Push The local repository doesn\u0026rsquo;t have a copy of q1.html or q2.html so I\u0026rsquo;ll tell bash \u0026ldquo;Hey, when I push, I want to push these files, too\u0026rdquo; with the command git add q1.html q2.html (and ultimately q3 as well).\nI\u0026rsquo;ve added my files, now I want to commit them, git commit -m \u0026quot;Complete Assignments\u0026quot;, where -m is called a flag saying \u0026ldquo;I want to include the message, \u0026lsquo;Complete assignment\u0026rsquo;\u0026rdquo;.\nFrom there, I can push my changes, git push origin master.\nOnce I push my changes, all five files (three *.Rmds and two *.htmls will push to the local repository.\nSo why are git add, git commit, and git push three different commands? It could be one right?\nThe first, I hope it\u0026rsquo;s a bit intuitive as to why git add is it\u0026rsquo;s own command. You don\u0026rsquo;t necessarily want to add ALL the files in a folder to the remote repo. There is a command, use it wisely, to add all new files git add . Similarly, to add all changes, added and deleted files, etc. There\u0026rsquo;s git add -A where dot and -A respectively are flags for \u0026ldquo;all new\u0026rdquo; and \u0026ldquo;all\u0026rdquo; changes.\nI\u0026rsquo;m not the only one whose committed a file, gotten the message that it was successful, then\u0026hellip; it\u0026rsquo;s not there. It\u0026rsquo;s not where it\u0026rsquo;s supposed to be! Why isn\u0026rsquo;t my file online? There\u0026rsquo;s some nuance in the difference between git commit and git push. The way I think of it is that git commit is kind of like putting a bookmark in my book, while git push is talking about the latest changes (between this time and the last time I dropped a bookmark in my book) with a(n obviously Zoom) book club or something larger than myself. Does that make sense? commit logs changes locally, while push updates our remote repository with our changes.\nFor this class, we only commit and push once, usually. But when you\u0026rsquo;re working on a larger project you can absolutely commit and push to a repo dozens or hundreds of times.\nEdit: Formatting, added git diff information\n","date":"April 16, 2020","permalink":"/docs/2020-04-16-git-commands/2020-04-16-git-commands/","section":"Docs","summary":"The original post is a response to a class forum.","title":"Git Commands"},{"content":"I\u0026rsquo;m excited. It\u0026rsquo;s been a long road trying to find the right platform to build on. Fortunately, hosting on GitHub Pages, along with the incredible development of the Jekyll-based Minimal Mistakes theme has made web-development a (relatively) seamless task for integrating my projects from GitHub. I\u0026rsquo;ll post source code and repositories as reasonable. Some of the projects are University-based and don\u0026rsquo;t change much year-to-year. In those cases, sorry kids. Check out the end result but we can\u0026rsquo;t make it that easy for you.\nThe last ten or so years have been filled with amazing experiences and collaborations. Why not showcase my work and make it easier to collaborate with those that share my interests? My projects get pretty diverse: spatial models and highly detailed analytical maps, integration of bioinformation and APIs, and sociological studies are the first to come to mind. And while the ability to show off some diversity is great, it\u0026rsquo;s just a jumbled mess if it\u0026rsquo;s not well organized.\nThis site will help organize those thoughts and projects. Blog posts, much like this one, will be rare and the presentation of projects and information will take center stage. Already in the works is a series on plain-language statistics and the conversion of existing projects into a more web-friendly format. Projects and posts will have searchable tags at the bottom of each page, which should allow you to find related materials. I\u0026rsquo;ll leave a small number of static pages at the top of each page with some of my background information as well.\n","date":"April 1, 2020","permalink":"/docs/2020-04-01-motivation-and-organization/2020-04-01-motivation-and-organization/","section":"Docs","summary":"I\u0026rsquo;m excited.","title":"Motivation and Organization"},{"content":"","date":"January 1, 0001","permalink":"/tags/about/","section":"Tags","summary":"","title":"about"},{"content":"","date":"January 1, 0001","permalink":"/categories/about/","section":"Categories","summary":"","title":"about"},{"content":" If quarantine brought me one gift, my once three-hour commute became three hours each day to build a portfolio and digress on some of the more interesting topics in statistics. This of course, comes at the expense of turning the dining room into my new office.\nHi, I\u0026rsquo;m Jes.\nI\u0026rsquo;m an institutional researcher with a background in Statistics and Data Science. In the office, my focus is on student equity through the use of data. I also love integrating Geographic Information Systems (GIS) as a visual and analytic aid in my work. Prior to institutional research, my work primarily focused on financial accounting and business administration. Outside of work, my projects are community-oriented and focus on voter advocacy and registration, community organization, and environmental awareness.\nBeautiful, Ethical Data #\rMy interest in statistics started after noticing a lot of confusion in how information was being shared. Projects, like annual reports made for the public, were often unclear. Ambiguity shows up in two forms: unethical data manipulation and poor visualization.\nThere seems to be a general misunderstanding that science can tell data what to say. Answers are drafted before our models were ever designed. \u0026ldquo;Have the data show\u0026hellip;\u0026rdquo;\nIt\u0026rsquo;s true that a single dataset can tell many different stories. Small shifts in how data are accessed, filtered, and modeled can change a narrative. This fact, however, can and will be problematic. We as statisticians minimize ambiguity through transparency in our choice of practices.\nAmbiguity is also what moves statistics away from pure mathematics. It\u0026rsquo;s an applied field that borrows from both art and science. These fields are naturally complementary. Your presentation of data is useless if it lacks clarity. Simplifying visuals and creating some consistency are simple tactics to improve visualization.\nSimple mistakes and malicious manipulation alike can alter data into something unrecognizable. Having the tools and network to call out those transgressions is the first step, whereas my focus comes in teaching folks to see the issues in data visualization and manipulation for themselves.\nPlain-Language Statistics #\rThere\u0026rsquo;s a lot of power in simplifying statistical language to meet an audience at their level of understanding. I love watching faculty reach an \u0026ldquo;aha\u0026rdquo;-moment with their own information through Q\u0026amp;A discussions. Similarly, teaching young students the fundamentals of object-oriented programming has given me the opportunity to assess how well I know the fundamentals myself. With both parties, I\u0026rsquo;m learning as much from them as they are from me.\nOut of Office #\rBoston Basin, North Cascades. Summer 2019\rI\u0026rsquo;ve been lucky enough in the past several years be able to dedicate my time out of the office to exploring many of our public lands. My profile photo was taken on the rim of a cinder cone volcano in Lassen National Park. The photo above is of one of my favorite spots on the west coast.\nIf I\u0026rsquo;m not in the state, I\u0026rsquo;m likely out exploring what the rest of the West has to offer. I\u0026rsquo;m looking forward to more days out of the house, out of the office, and in the backcountry.\n","date":"January 1, 0001","permalink":"/about/","section":"jeslacourse","summary":"If quarantine brought me one gift, my once three-hour commute became three hours each day to build a portfolio and digress on some of the more interesting topics in statistics.","title":"About the Author"},{"content":"","date":"January 1, 0001","permalink":"/tags/classification/","section":"Tags","summary":"","title":"classification"},{"content":"","date":"January 1, 0001","permalink":"/categories/classification/","section":"Categories","summary":"","title":"Classification"},{"content":"","date":"January 1, 0001","permalink":"/tags/coursework/","section":"Tags","summary":"","title":"coursework"},{"content":"","date":"January 1, 0001","permalink":"/tags/datasets/","section":"Tags","summary":"","title":"datasets"},{"content":"","date":"January 1, 0001","permalink":"/categories/datasets/","section":"Categories","summary":"","title":"datasets"},{"content":"Datasets\nBirth Data - Forked from /538/data/\nhttps://github.com/jeslacourse/data/blob/master/births/US_births_2000-2014_SSA.csv\n","date":"January 1, 0001","permalink":"/datasets/","section":"jeslacourse","summary":"Datasets","title":"Datasets"},{"content":"\rWorking with community-oriented and socially conscious organizations to promote social awareness through meaningful data interpretation and visualization\nInstitutional Research Highlights\nTODO: Let\u0026rsquo;s share some projects and templates! Data Management and Statistical Highlights\nCreation and implementation of voter outreach GIS database for California\u0026rsquo;s 20,000 precincts. Adopted in part by major 2016 Presidential candidate campaign\nPrimary role in data management/conversion project of approximately 500,000 entries\nEarned commendations and national recognition from top management within firm\nCustom built an intuitive, dynamic vehicle maintenance workbook for 12 vehicle fleet with accompanying Fleet Procedures Manual\nCommunity Service and Leadership Highlights\nExtensive and diverse 15+ years community service track record with emphasis on youth and voter advocacy within Placer County\nServed as interim administrative supervisor and team lead through peak tax season during merger with nation\u0026rsquo;s 8th largest financial firm\nDeveloped plan of action, provided direct communication with office and regional management on immediate hiring needs\nDirect involvement in hiring and training to build a highly adept administrative team\nTeam commendation from Managing Principal for multiple seamless work seasons, citing promotion of open, positive intraoffice communication and unparalleled efficiency\nPROFESSIONAL EXPERIENCE #\rSenior Research Analyst (Jul 2021 - ) #\rResearch Analyst (Jul 2020 - Jul 2021 ) #\rR-based Institutional Research\nCode Coach (Instructor) (Dec 2019 - Jan 2022) #\rDevelop personalized, project-based curriculum for individuals and small groups of students aged 7-16\nPrimary Languages: Python, C++, C#\nAdapted coursework to be taught remotely in private and/or small group sessions\nSenior Administrator (Jan 2015 – Sept 2018) #\rAccounts Payable Clerk II (Sep 2012 – Jan 2014) #\rCensus Follow-Up Enumerator/Interviewer (April – July 2010) #\rVOLUNTEER WORK #\rVoter Advocate 2015 - 2016, 2019 #\rNorthern California GIS mapping support for Presidential primary candidate\nDistrict 4 (Sacramento through Central Sierras) and District 13 (Oakland) support [CA CD4 \u0026amp; CD16; 2016]\nOrganization of teams for door-to-door community interaction to promote voter registration and candidate awareness\nEmphasis on voter rights information, particularly no party preference voting rights during CA semi-open primaries\nWashoe County, NV Caucus observer and voting advocate\nCommunity Advocate, Loomis, CA 2014 - 2018 #\rDirect outreach to approximately 500 community members through local organization and public comments during Town Hall meetings\nEmphasis: Oil train disaster preparedness and housing development impacts on existing infrastructure\nLoomis General Plan annotation and distribution, working to promote awareness of local issues and current events to promote Town Council attendance and interaction with community leaders\nYouth Advocate, Garden Valley, CA Dec 2007 - 2009 #\rHead of interview and surveying committee, film editing and organization\nAttended meetings with members and partners of the non-profit film organization Tower of Youth\nEDUCATION #\rUniversity of California, Davis, CA #\rB.S. Statistics - Data Science Track\nComputer Science Minor, Out in STEM Club Member\nRelevant Coursework\nSierra College, Rocklin, CA #\rA.S. Business Administration | A.S Accounting | A.S. Natural Sciences\nGeographic Information Systems Certificate\nPROGRAMMING \u0026amp; SOFTWARE EXPERIENCE #\rProgramming Comprehension #\rR with RMarkdown (2018-)\nSQL, including SQL Server and PostgreSQL (2015-)\nOOP programming comprehension (2014-)\nPython (2016-) C++, Java, Javascript (1 year) HTML \u0026amp; CSS3 (2015-)\nHugo for Blogdown (2022 -) Jekyll in Ruby for Web Development (2019-2022) Specialized Software and Skills #\rType Speed: 55 AWP and 10 key by touch\nFormatting: Markdown, HTML/CSS3\nRStudio (Posit) for Analysis\nArcGIS Desktop 10.1+ and Online: Mapping, spatial analytics, and data visualization resource\nJupyter Notebooks (Anaconda Suite)\nMicrosoft Office Suite: 10+ years professional experience working with Excel, Word, Outlook, PowerPoint and Access\nAccess: Proficient; join \u0026amp; relationship competency, table building, queries, data integrity\nExcel: High proficiency; building and customizing workbooks, vlookup \u0026amp; filter tables\nWord: High proficiency; business report writing, template customization, automation\n","date":"January 1, 0001","permalink":"/experience/","section":"jeslacourse","summary":"Working with community-oriented and socially conscious organizations to promote social awareness through meaningful data interpretation and visualization","title":"Experience"},{"content":" Abstract\nFrequently, k-nearest neighbors classification is applied with features chosen arbitrarily, while `k` is adjusted to improve the accuracy of the model. For this experiment, `k` is fixed. Using correlations with the strict acknowledgement that all features are continuous, a feature set with high correlation within itself is selection. Visual analysis using density and scatter plots show that these features also share distinct distributions and thusly make great candidates for k-nearest neighbors clustering. The four features return a well classified set, while tweaking the set by removing features with less distinctive groupings returns very high accuracy with an increased risk of overfit. Both features sets perform better than using all features with the same fixed hyperparameter. Introduction #\rWith a small database, it\u0026rsquo;s easy enough to fit all features to a model, or select all quantitative data, then frequently modify the hyperparameters to boost up the accuracy. But what information are we losing in the process?\nFeature selection is just as important as hyperparameter choice, yet many classifiers cherry-pick features without much thought for their empirical basis. The focus of this experiment will look at the impact of feature selection on wheat kernel classification. Given the distinctive groupings, that occur within many features, k-nearest neighbors will be used for classification. While the hyperparameter, k will be easier to tweak, we must account for the impact of boosting hyperparameters on the risk of overfit.\nData Information and Attributes #\rData Source #\rWheat seed data provided by UCI’s Center for Machine Learning and Intelligent Systems.\nhttps://archive.ics.uci.edu/ml/datasets/seeds#\rData Information and Attributes #\rThere are 210 sample kernels of three varieties of wheat, Kama, Rosa, and Canadian.\nEach wheat kernel is assigned seven attributes: grain area, perimeter, compactness, length and width of kernel, asymmetry coefficient and length of kernel groove.\nEach measurement is given in millimeters.\nData Structure and Cleaning #\rOur source data is a text file with headless, tab-separated data (Table 1).\nTable 1: Raw Data Input\rV1\rV2\rV3\rV4\rV5\rV6\rV7\rV8\r15.26\r14.84\r0.8710\r5.763\r3.312\r2.221\r5.220\r1\r14.88\r14.57\r0.8811\r5.554\r3.333\r1.018\r4.956\r1\r14.29\r14.09\r0.9050\r5.291\r3.337\r2.699\r4.825\r1\rTo improve readability and analysis, each feature is assigned a descriptive name. The response variable, ‘wheat‘, is factored into a categorical variable with levels \u0026ldquo;Kama\u0026rdquo;, \u0026ldquo;Rosa\u0026rdquo;, and \u0026ldquo;Canadian\u0026rdquo; replacing numeric variables 1,2, and 3, respectively (Table 2).\nTable 2: Legible Dataframe\rarea\rperimeter\rcompactness\rlength\rwidth\rasymmetry\rgroove\rwheat\r15.26\r14.84\r0.8710\r5.763\r3.312\r2.221\r5.220\rKama\r14.88\r14.57\r0.8811\r5.554\r3.333\r1.018\r4.956\rKama\r14.29\r14.09\r0.9050\r5.291\r3.337\r2.699\r4.825\rKama\r13.84\r13.94\r0.8955\r5.324\r3.379\r2.259\r4.805\rKama\r16.14\r14.99\r0.9034\r5.658\r3.562\r1.355\r5.175\rKama\r14.38\r14.21\r0.8951\r5.386\r3.312\r2.462\r4.956\rKama\rMissing Values #\rUpon initial inspection, the data is complete and contains no missing values.\n{: .align-center}\nFigure 1. Outlier and Quantile Analysis by Feature Two or three Kama kernel appears to fall outside the normal distribution in several categorical factors (Fig 1), but they are not far enough away from the rest of the level to warrant removal. With no major outliers and no missing values, data cleaning was a relatively simple process involving only the adjustment of variate names and factor levels.\nVisualizations and Analysis #\rThe wheats create distinctive groups when classified by area and perimeter. Length and width also provide significant groupings, while compactness, asymmetry, and groove have substantial overlap between the wheat variants (Fig 2).\nFigure 2: Histograms with Density Overlays by Feature\nWhen classifying by wheat groove, in particular, the wheats fall into two distinctive groups, with Canadian and Kama wheats sharing nearly identical distributions. The groove on Rosa wheat is significantly longer and has almost no overlap with the opposing groups.\nCorrelation and Associativity #\rWith no categorical data, we can create a table of comparative values between all of our variates (Fig 3).\nFigure 3: Correlation and Associativity Area is highly correlated with the perimeter (r = 0.994), length (r=0.950), and width (r=0.971) of a given kernel. These values are highly correlated with each other as well. Another notable factor, the kernel groove, is also highly correlated with the four noted factors.\nClassification #\rGiven the distinctions in distribution, as well as the high correlation between the given factors, k-nearest neighbors with factors area, perimeter, length, and width as the classifying features. All features are given in millimeters and will not be standardized.\nAll experiments are completed in R with class::knn, k=5. The train-test ratio will be set at 80/20 to provide an adequate test sample size, n=42. Training set, n=168.\nK-Nearest Neighbors with Selected Features #\rTable 3: Confusion Matrix: Classification with Area, Perimeter, Height and Width\rPred/Acc Kama\rRosa\rCanadian\rKama\r11\r1\r0\rRosa\r1\r14\r0\rCanadian\r2\r0\r13\rThe initial classification did well, 90.47% (Table 3). Two issues stand out: (a) Kama kernels are misclassified relatively frequently, and (b) other kernels are misclassified as Kama kernels relatively frequently. These results are relatively intuitive, as the peak of the distribution for Kama kernels falls between Rosa and Canadian kernels for the factors area, perimeter, length, and width.\nThe Kama distribution tails for length and width, as noted before overlap significantly. There is very little to no misclassification between Rosa and Canadian wheat kernels as the factor distributions and means are consistently more distant from each other when compared to the difference in means with Kama kernels for either wheat.\nK-Nearest Neighbors with Minimal Features #\rTable 4: Confusion Matrix: Classification with Area and Perimeter\rPred/Acc Kama\rRosa\rCanadian\rKama\r13\r1\r0\rRosa\r0\r14\r0\rCanadian\r1\r0\r13\rRemoving height and width as factors increased the accuracy of our knn prediction to 95.24%, but that number on it’s own raises some suspicion (Table 4). The highly accurate result is likely more accidental, but it does show us that these features with more overlap in the tails do have a significant impact on the number of misclassified values.\nK-Nearest Neighbors with All Features #\rTable 5: Confusion Matrix: Classification with All Features\rPred/Acc\rKama\rRosa\rCanadian\rKama\r11\r1\r0\rRosa\r2\r14\r0\rCanadian\r1\r0\r13\rAs a control, we can train with all seven features. Given the significant overlap in compactness, asymmetry, and groove, the accuracy is expected to be lower than when strictly training with area and perimeter. The results are as expected, 90.47% (Table 5).\nConclusion #\rThe experiment shows the value of data exploration when choosing features. The experiment also shows that increasing the number of features doesn’t necessarily improve the accuracy of testing. Taking care to minimize inadvertent p-hacking should always be at the forefront of a researcher’s mind when building experiments.\nWith that said, the analysis is based off of a single fixed training and test sample. Improvements can be made to the experiment by replicating the analysis with cross validation, bootstrapping, or k-folds to ensure replicability.\n","date":"January 1, 0001","permalink":"/docs/2020-05-17-feature-choice-in-classification/2020-05-17-feature-choice-in-classification/","section":"Docs","summary":"Abstract","title":"Feature Choice in Classification of Kernels"},{"content":"\rShow code\r# calculating user coordinates x \u0026lt;- rnorm(100) + 2 y \u0026lt;- 2 * x + 1.5 * rnorm(x) + 2 plot(jitter(x), y, pch = 20, xlab = \u0026#34;Pages\u0026#34;, ylab = \u0026#34;Awesomeness\u0026#34;, main = \u0026#34;You Are Here\u0026#34;) Oops, let\u0026rsquo;s get you back on track \u0026mdash; Try searching by tag or category.\n","date":"January 1, 0001","permalink":"/404/","section":"jeslacourse","summary":"Show code\r# calculating user coordinates x \u0026lt;- rnorm(100) + 2 y \u0026lt;- 2 * x + 1.","title":"Page Not Found"},{"content":"","date":"January 1, 0001","permalink":"/tags/projects/","section":"Tags","summary":"","title":"projects"},{"content":"\rIt\u0026rsquo;s Fall, my creative time. I\u0026rsquo;ll be updating in the near future with some of my more recent IR work.\rB.S. Statistics - Data Science Track\nMinor - Computer Science\nHighlights #\rBig Data \u0026amp; High Performance Statistical Computing #\rHigh-performance computing in high-level data analysis languages; different computational approaches and paradigms for efficient analysis of big data; interfaces to compiled languages; high-level parallel computing; MapReduce; parallel algorithms and reasoning. Python. R. SQL.\nCategorical Data #\rVarieties of categorical data, cross-classifications, contingency tables, tests for independence. Multidimensional tables and log-linear models, maximum likelihood estimation; tests of goodness-of-fit. Logit models, linear logistic models. Analysis of incomplete tables. Packaged computer programs, analysis of real data. R.\nProject: Byssinossis (Farmer\u0026rsquo;s Lung Disease) Determination. Link TBU (Apr 2020).\nData and Web Technologies for Data Analysis #\rEssentials of using relational databases and SQL. Processing data in blocks. Scraping Web pages and using Web services/APIs. Basics of text mining. Interactive data visualization with Web technologies. Computational data workflow and best practices. Statistical methods. Team lead. R. SQL.\nProject: Temporal Birth Trends (2000-2015). Link TBU (Apr 2020).\nDatabase Systems #\rTeam development and testing of L-Store style database system. Database modeling and design (E/R model, relational model), relational algebra, query languages, file and index structures, query processing, transaction management. Python. SQL.\nProject: JellyDB - GitHub\nMachine Learning #\rSupervised \u0026amp; unsupervised learning, including classification, dimensionality reduction, regression \u0026amp; clustering using modern machine learning methods. Applications of machine learning in biology (oncology) and engineering. Python.\nComputational Foundation and Focus #\rAgent-Based Modeling #\rAgent-based computer simulation and analysis with emphasis on learning how to model animals, including humans, to achieve insight into social and group behavior. Referred by instructor for continued studies in mate-choice and evolutionary game-theory modeling. Java.\nProject: Effects of Similiarity and Attractiveness in Mate Selection. Link TBU (Apr 2020).\nAlgorithm Design and Analysis #\rComplexity of algorithms, bounds on complexity, analysis methods. Searching, sorting, pattern matching, graph algorithms. Algorithm design techniques: divide-conquer, greedy, dynamic programming. Approximation methods. NP-complete problems. Python.\nComputational Linguistics #\rUnderstanding the nature of language through computer modeling of linguistic abilities. Relationships between human cognition and computer representations of cognitive processing.\nPractice in Data Science #\rPrinciples and practice of interdisciplinary, collaborative data analysis; complete case study review and team data analysis project. R.\nStatistical Data Science #\rIntroduction to computing for data analysis and visualization, and simulation, using a high-level language. Computational reasoning, computationally intensive statistical methods, reading tabular and non-standard data. R.\nStatistical Foundation #\rANOVA #\rFoundational experiment design. One- and Two-way ANOVA. Random effects modeling. R.\nProject: Bird-Nest Size Relationships. Link TBU (Apr 2020).\nApplied Linear Algebra #\rExtensive Problem Solving. Applications of linear algebra; LU and QR matrix factorizations, eigenvalue and singular value matrix decompositions. Matlab.\nProject: Classification of Handwritten Digits. Link TBU (Apr 2020).\nMathematical Statistics #\rSampling, methods of estimation, bias-variance decomposition, sampling distributions, Fisher information, confidence intervals, and some elements of hypothesis testing.\nTesting theory, tools and applications from probability theory, Linear model theory, ANOVA, goodness-of-fit.\nMultivariate Analysis #\rMultivariate normal distribution; Mahalanobis distance; sampling distributions of the mean vector and covariance matrix; Hotellings T2; simultaneous inference; one-way MANOVA; discriminant analysis; principal components; canonical correlation; factor analysis. Intensive use of computer analyses and real data sets. R.\nProbability Theory #\rFundamental concepts of probability theory, discrete and continuous random variables, standard distributions, moments and moment-generating functions, laws of large numbers and the central limit theorem. R.\nRegression Analysis #\rSimple and multi linear regression, variable selection techniques, stepwise regression, analysis of covariance, influence measures. R.\nStatistical Learning #\rFundamental concepts and methods in statistical learning with emphasis on supervised learning. Principles, methodologies and applications of parametric and nonparametric regression, classification, resampling and model selection techniques. Python.\nReferences: #\rUC Davis College of Letters and Science, Department of Statistics - Course Descriptions\nUC Davis College of Engineering, Department of Computer Science - Course Description\n2019-2020 General Catalog\n","date":"January 1, 0001","permalink":"/projects/","section":"jeslacourse","summary":"It\u0026rsquo;s Fall, my creative time.","title":"Projects"},{"content":"","date":"January 1, 0001","permalink":"/tags/work-experience/","section":"Tags","summary":"","title":"work experience"}]