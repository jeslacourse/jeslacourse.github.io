[{"content":"\r\n\r\n\r\n","date":"26 November 2022","permalink":"/docs/2022-11-26-diagnostic-plots-in-r/","section":"Docs","summary":"","title":"Diagnostic Plots in R"},{"content":"","date":null,"permalink":"/docs/","section":"Docs","summary":"","title":"Docs"},{"content":"","date":null,"permalink":"/","section":"jeslacourse","summary":"","title":"jeslacourse"},{"content":"","date":null,"permalink":"/tags/dataviz/","section":"Tags","summary":"","title":"Dataviz"},{"content":"","date":null,"permalink":"/tags/ggplot/","section":"Tags","summary":"","title":"Ggplot"},{"content":"\rA Quick Recap\rTable 1: Blood Pressure Sample\rPerson\r1\r2\r3\r4\r5\r6\r7\r8\r9\r10\r11\r12\r13\r14\r15\r16\r17\r18\r19\r20\rAge\r58\r46\r56\r37\r23\r30\r20\r25\r31\r50\r34\r27\r22\r52\r20\r42\r20\r58\r24\r35\rBP\r167\r136\r116\r116\r101\r121\r103\r101\r105\r158\r117\r118\r113\r148\r105\r142\r89\r153\r122\r135\rWe’re looking at the relationship between blood pressure samples and age. For this study, we have 20 people ranging in age between 20 and 58. Our median age is 32.5; mean age is a little higher at 35.5. We also determined that the blood pressure samples are evenly distributed.\nWe found evidence that our linear model was underestimating blood pressure rates for older participants. There was also an individual, Person 3 (56,113), whose blood pressure was substantially lower than expected.\nModel Fit\rThis article is all about the summary read out. Let’s take another look at the summary data for our model:\nsummary(m)\r## ## Call:\r## lm(formula = BP ~ Age, data = data)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -34.248 -6.525 1.099 8.124 15.639 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 76.6333 7.9199 9.676 1.48e-08 ***\r## Age 1.3146 0.2091 6.288 6.28e-06 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 12.36 on 18 degrees of freedom\r## Multiple R-squared: 0.6872, Adjusted R-squared: 0.6698 ## F-statistic: 39.54 on 1 and 18 DF, p-value: 6.284e-06\rR modeling stores a handful of features with the saved variable, these can be accessed by calling names(m). We used the coefficients in the last article, now we’ll take a look at attributes that give us a better look at how well our model fits the data.\nnames(m)\r## [1] \u0026quot;coefficients\u0026quot; \u0026quot;residuals\u0026quot; \u0026quot;effects\u0026quot; \u0026quot;rank\u0026quot; ## [5] \u0026quot;fitted.values\u0026quot; \u0026quot;assign\u0026quot; \u0026quot;qr\u0026quot; \u0026quot;df.residual\u0026quot; ## [9] \u0026quot;xlevels\u0026quot; \u0026quot;call\u0026quot; \u0026quot;terms\u0026quot; \u0026quot;model\u0026quot;\rResiduals\rOur residuals, or differences between the estimated and actual blood pressure measurements, range from an overestimate of 34.2 mmHg, to an underestimate of 15.6 mmHg. The median residual is 1.1mmHg, a slight underestimate. We can determine which residuals belong to a sample by calling resid(m), the equivalent of m$residuals. Our min and max residuals belong to Person 3 and Person 10, respectively.\nTable 2: Blood Pressure Sample\r1\r2\r3\r4\r5\r6\r7\r8\r9\r10\r11\r12\r13\r14\r15\r16\r17\r18\r19\r20\r14.12\r-1.1\r-34.25\r-9.27\r-5.87\r4.93\r0.08\r-8.5\r-12.38\r15.64\r-4.33\r5.87\r7.45\r3.01\r2.08\r10.16\r-13.92\r0.12\r13.82\r12.36\rThese values may be difficult to read on their own, so I’ll add them back to our dataframe and visualize.\ndata$Predicted \u0026lt;- predict(m)\rdata$Residuals \u0026lt;- resid(m)\rWe can now match up the residuals in the table above with the estimates in the figure below. For the figure below, black dots are the actual values, while the black rings along the regression line are the estimated values given the subject’s age. Person 3 and Person 10 are not well estimated, while Person 2, 7, 14, and 18 have a much smaller residual error.\nFigure 1: Residuals: Over- and Underestimates of Blood Pressure\rResidual Standard Error (RSE)\rThe RSE is a standard deviation estimate for linear regression. Smaller values tend to mean a better fit, while an RSE of zero (our estimates perfectly predict our values) would certainly mean our model has been overfit.\n\\[RSE = \\sqrt{SSE/df_R}\\]\rThe residual standard error is the square root value of the sum of squared errors (SSE) divided by the residual degrees of freedom (df). Squaring residuals eliminates the sign (\\(\\pm\\), for an over- or underfit). Taking the mean of residuals otherwise would simply give us a value at or near zero as over- and underestimates cancel each other out. Given 20 samples and 1 parameter (Age), we’ll calculate the degrees of freedom using \\(df_R = n - p - 1\\) or \\(df_R = 20 - 1 - 1 = 18\\).\nSSE \u0026lt;- sum(resid(m)^2) # 2751.189\rdf \u0026lt;- length(m$residuals) - length(m$coefficients) # 18\rRSE \u0026lt;- sqrt(SSE/df) #12.363\rMultiple R-squared\rMultiple R-Squared and the Adjusted R-squared focus on correlation in our model. Correlation can range from -1 to 1, with values closer to -1 or 1 resulting in high correlation. For our data, we have a correlation value of 0.829 (cor(Age,BP)) which gives us a multiple R-squared value of 0.687. Adjusted R-squared values are calculated to account for increased variance with multiple parameters and tend to be more conservative. As such, Adjusted R-squared values are particularly useful for models with several parameters.\ncor(data$Age, data$BP)^2 # 0.687\rF-statistics\rLinear modeling in R uses t-testing and F-statistics to determine if any coefficients significantly influence our dependent variable. In our case, we are testing out whether Age influences BP. In short, a p-value \u0026lt; 0.05 would tell us that Age is a significant coefficient at \\(\\alpha = 0.05\\). In our case, we can see that the p-value for Age is \u0026lt; 0.001; there is a relationship between age and blood pressure.\npvals \u0026lt;- summary(m)$coefficients[,4] # 6.28e-06 In Summary\rThe summary() feature provides critical feedback about the relationship between our dependent variable, BP and our coefficient or independent variable, Age. While not perfect, we have a relatively good model for predicting blood pressure.\nThe next post will look at how to improve our model fit. We’ll use R’s built in diagostics to determine the impact of outliers, sample size, and variance within the data.\n","date":"13 November 2022","permalink":"/docs/2022-10-06-errors-and-residuals-with-r/","section":"Docs","summary":"Reading \u003ccode\u003elm()\u003c/code\u003e outputs and residual analysis for linear models","title":"Model Fit and Residuals with R"},{"content":"","date":null,"permalink":"/tags/plotly/","section":"Tags","summary":"","title":"Plotly"},{"content":"","date":null,"permalink":"/tags/regression/","section":"Tags","summary":"","title":"Regression"},{"content":"","date":null,"permalink":"/tags/rstats/","section":"Tags","summary":"","title":"Rstats"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":null,"permalink":"/categories/regression/","section":"Categories","summary":"","title":"Regression"},{"content":"\rFitting a Line to Data\rSimple linear regression a method of estimating an outcome based on a single related variable. We’ll want to estimate the systolic blood pressure (our outcome) for 20 subjects based on their age (an independent variable) using R.\nHere are our 20 subjects:\nTable 1: Blood Pressure Sample\rPerson\r1\r2\r3\r4\r5\r6\r7\r8\r9\r10\r11\r12\r13\r14\r15\r16\r17\r18\r19\r20\rAge\r58\r46\r56\r37\r23\r30\r20\r25\r31\r50\r34\r27\r22\r52\r20\r42\r20\r58\r24\r35\rBP\r167\r136\r116\r116\r101\r121\r103\r101\r105\r158\r117\r118\r113\r148\r105\r142\r89\r153\r122\r135\rFirst the Basics\rIt’s good to get a feel for the characteristics of our sample. Our subject information is stored in a dataframe called data.\nWe’ll want to collect some fundamental information about our sample.\nCall summary(data):\n## Person Age BP ## Length:20 Min. :20.00 Min. : 89.0 ## Class :character 1st Qu.:23.75 1st Qu.:105.0 ## Mode :character Median :32.50 Median :117.5 ## Mean :35.50 Mean :123.3 ## 3rd Qu.:47.00 3rd Qu.:137.5 ## Max. :58.00 Max. :167.0\rThe summary tells us we have 20 subjects with a mean age of \\(35.5\\) years. Looking at the quartiles, then our minimum and maximum values, the ages appear to be evenly distributed from 20 to 58 years. A quick check of our blood pressure data shows values that are evenly distributed as well. We’ll look at formal diagnostics later, but right now linear regression is looking like a good tool for estimation.\nVisualizing Data\rThe function ggplotly() from the plotly library is a fantastic way to convert ggplot() items into interactive figures. Note that you can simply use the ggplot() code in the brackets for a static plot.\n{\rggplot(data, aes(name = Person, x = Age, y = BP )) + geom_point() + labs(title = \u0026quot;Blood Pressure by Age, n = 20\u0026quot;, y = \u0026quot;Blood Pressure (mmHg)\u0026quot;, x = \u0026quot;Age (Years)\u0026quot;)\r} %\u0026gt;% ggplotly()\rFigure 1: Blood Pressure by Age, n= 20\rWe certainly have a linear trend. There are slightly more younger subjects than older subjects. Generally speaking, there doesn’t appear to be too much spread (variability) in our data. That said, one of our subjects, Person 3, appears to have a systolic blood pressure substantially lower than expected at for their age; 116 mmHg at the age of 56.\nModeling\rWe appear to be in good shape for modeling with linear regression. Simple linear regression is presented in point-slope form, with parameters \\(\\beta_0\\) and \\(\\beta_1\\) making up our slope and intercept, respectively.\n\\[y = \\beta_0 + \\beta_1x\\]\nIn R, we’ll call lm() for linear modeling. In this case, we want to model blood pressure BP by age.\nlm(formula = BP~Age, data) -\u0026gt; m\r## ## Call:\r## lm(formula = BP ~ Age, data = data)\r## ## Coefficients:\r## (Intercept) Age ## 76.633 1.315\rThe function returns two coefficients, y-intercept (Intercept) and a ‘slope’ for variable, Age. Starting at 76.633 at Age = 0, we can expect a 1.315 mmHg increase in systolic blood pressure with each added year.\nFollowing the regression formula, we’ll set (Intercept) to \\(b_0\\) and Age to \\(b_1\\).\n# Set coefficients to betas\rb0 \u0026lt;- m$coefficients[1] # 76.633\rb1 \u0026lt;- m$coefficients[2] # 1.315\r\\[BP_{est} = b_0 + b_1*Age\\\\\rBP_{est} = 76.663 + 1.315*Age\\]\nWe can now add the regression line to the plot.\n{\rggplot(data, aes(x = Age, y = BP)) +\rgeom_point() + geom_line(aes(y = b0 + b1*Age)) + labs(title = \u0026quot;Estimated Regression Line: Blood Pressure by Age\u0026quot;)\r} %\u0026gt;% ggplotly()\rFigure 2: Regression Line: Blood Pressure by Age\rHovering along the line, we can find the estimated blood pressure for any age in the range.\nFit and Validity\rNote: In this case, (20,103) and (58,153) appear to begin and end the regression line. While it occurs for these data, it’s not common for a regression line to start and end with “true” subject values.\nNotice how the regression line appears to sit in the middle of the points for younger subjects, but after x=40 the line tends to fall under the points. While the regression line appears to be a good fit for the younger subjects, our model frequently underestimates the blood pressure of older subjects.\nSo how good of an estimate is our line? The next post will talk more about residual errors, goodness-of-fit, and diagnostics.\n","date":"2 October 2022","permalink":"/docs/2022-10-02-simple-regression/","section":"Docs","summary":"An introduction to linear regression with \u003ccode\u003elm()\u003c/code\u003e in R","title":"Simple Regression with R"},{"content":"\rSierra College\rProject Scope #This study tracks the impact of course-taking and change-in-major habits on award-earning and transfers for the 224 Biology for Transfer majors in the 2017-18 cohort.\nThe Associate in Science in Biology for Transfer degree (AS-T) prepares students to transfer into the CSU system to complete a bachelor’s degree in biology, or a major deemed similar by a CSU campus. This program provides students with a strong foundation in biology. — Biological Sciences. Sierra College. 2021.\nLooking back on the 2017-18 cohort allows for adequate tracking of the degree-earning process. For consistency, students are tracked based on their last declared major.\nKey Findings and Action Items #Persistence and Award-Earning\nBiology for Transfer Majors: Years to Completion, First Award or Transfer\r1 2 3 4+ No Award Total Distinct Students 1 9 11 56 147 224 First-year persistence is low for this cohort.\nOf new 224 Biology for Transfer majors in the 2017-18 cohort, 36.6% (n:82) persisted from their first term into coursework a year later. With that said, 77 (34.38%) earned an award or transferred within four years.\nOverall, 28 students earned an award or certificate. 60 students transferred to a four-year college, and 44 students transferred without an award.\nJust 23.3% (14 of 60) students completed degree-required Physics coursework before transferring. 58.3% (35 of 60) completed some Biology coursework before transferring with a total of 38.1% (23 of 60) completing BIOL 2 or 3.\nSummer Coursework is Critical\nEvery student who earned an AS Natural Science or AS-T Biology took at least one CHEM and/or PHYS course during a Summer session.\nStudent Preparedness Clashes With Course Availability\nOnly 20.1% of students took a major course in their first year. Taking major coursework in the first year led to a 29.3% point increase in award-earning, 55.6% of students taking first-year major coursework earn an award, compared to 26.3% without first-year major coursework. Incoming students appear to be well prepared for college-level coursework. The majority of first-year major enrollments were made up of MATH 30 (n:19) and CHEM 1A (n:17), followed up by CHEM 3A (n:11).\nStudents who didn\u0026rsquo;t enroll in Chemistry their first year but persisted to next Fall typically enrolled in CHEM 1A or CHEM 3A during their second year or during a Summer session. Preliminary findings from a concurrent study on entry-level Chemistry coursework indicate that late registration dates for new cohorts may be playing a key factor in limiting prepared students from enrolling in first-year CHEM 1A and CHEM 3A. Similar trends with course availability play out near the end of the student journey as students attempt to enroll in PHYS 105\\L and 110\\L.\nMajor Courses\nEarly enrollment in CHEM 1A or CHEM 3A and/or MATH 27 or MATH 30 are the the best early indicators that a Biology for Transfer student is on track.\nStudents who enroll in PHYS 110/L, BIOL 2, and/or BIOL 3, regardless of outcome, have successfully navigated several prerequisite barriers and have a nearly guaranteed chance of earning an award. With that said, the number of students who make it to these courses is relatively low. Nearly all students who enrolled in Physics opted for the PHYS 100 series in lieu of the 200 series. MATH 30 is highly preferred by this cohort over MATH 16A.\nGeneral Education\nGeneral education completion is not a major concern for this cohort. Enrollment in IGETC 5A|B Physical and Biological Science coursework outside of their degree coursework, i.e. ANTH 1 or Academic Plan-aligned courses ESS 1 and GEOG 3, tend to have a lower chance of award-earning or transfer.\nThe Program #Requirements and Prerequisites #Link: Sierra College Catalog, 2020-21\nThe Biology for Transfer degree is prerequisite heavy: In order to complete BIOL 2 or 3, students must take at least three consecutive semesters of degree-applicable coursework. Similar trends develop for completion of the PHYS 100 and 200 series.\nHidden Units: Students taking the CHEM 3 series in lieu of CHEM 1A must enroll concurrently in CHEM X|Y. The additional corequisite increases the overall unit load from 6 units over two semesters to 10 units.\nPersistence #Of 224 Biology for Transfer majors, 63.4% (n:142) persisted onto their second term. After a year, that number drops to 36.6% (n:82).\nOf the 82 students who continued to their second term: 87.8% (n: 72) completed their English requirement, 81.7% (n: 67) completed college math, and 52.4% (n: 43) completed college level chemistry.\nOf the 142 students who didn\u0026rsquo;t continue to their second term: 54.6% (n: 64) completed their English requirement, 43.3% (n: 61) completed college math, and 15.6% (n: 22) completed college level chemistry.\nStill in the Game\nSeventeen students from the 2017-18 cohort have enrolled in coursework during the 2020-21 academic year. The last major courses to be completed include BIOL 1 (n:5), PHYS 110|L (n:5), BIOL 2 (n:4), BIOL 3 (n:3), and CHEM 1B (n:3).\nAwards #By 2020-21, 28 students earned an award or certificate. Twenty-seven of the 28 award earners held at least one AS Degree and one student earned a certificate. Eighteen students earned an AS-T Biology, all of whom picked up an AS Natural Science award along the way.\nEvery student that earned an AS-T Biology or AS Natural Sciences took Chemistry or Physics coursework over the summer. The mean completion time for completers in the 2017-18 cohort is just over three years including one or more summer sessions.\nThe Great Bottleneck (And the Summer Savior) #All 27 Biology for Transfer majors who earned an AS Degree took at least one CHEM or PHYS course over a Summer session. Similarly, 61.6% of the 60 students who ultimately transferred took least one course over Summer. Thirty-two of the 37 students, 86.9%, who took Summer courses before successfully transferring took at least one CHEM or PHYS over the Summer.\nSuccessful students need to complete MATH and CHEM coursework early in their academic plan. Of the 82 students who continued to their second term, 81.7% (n: 67) completed college math and 52.4% (n: 43) completed college level chemistry. In contrast, of the 142 students who didn’t continue to their second term, 43.3% (n: 61) completed college math and 15.6% (n: 22) completed college level chemistry.\nSkipping the Award #Ten students earned an award or transferred in two years. After four years, 77 (34.38%) students earned an award or transferred. The majority of the 74 four-year college transfers did so with out completing the AS-T; 49 (66.2%) students transferring without award.\nCrosstabs: Award- and Transfer-Earning by Headcount\rNo Transfer 4-Year Transfer Has an Award 12 16 28 No Award 152 44 196 164 60 224 A Difference in Trends: Transferring With or Without an Award #Students who transfer without an award enrolled in fewer major courses and tend to skip Physics coursework before moving to a four-year college.\nTransfer Students: Frequently Skipped Coursework\rTook Any Physics Any Biology Biology 2 or 3 Total Transfers Headcount 14 35 23 60 Proportion 23.3% 58.3% 38.1% 100.0% Overall, 23.3% (14 of 60) students completed Physics coursework before transferring. 58.3% (35 of 60) completed some Biology coursework before transferring with a total of 38.1% (23 of 60) completing BIOL 2 or 3. Transfer students are also more likely to enroll in BIOL 3 (n:22) over BIOL 2 (n:19) and are less likely to pass their coursework on their first attempt.\nRepeating Key Courses #Students transferring without an award tend to complete fewer units than award-earners.\nStudents who repeat BIOL 1 and CHEM 1A tend to earn an award before transferring. Both award and non-award earning transfer students are more likely to repeat MATH 30 than any other course.\nFirst-Year Course-Taking Trends #Frequently Taken Courses #\rMajor Courses Taken During First Year\rTook Major Coursework Distinct Students Prop Awards Prop Yes 45 20.1% 25 55.6% No 179 79.9% 47 26.3% Total 224 - 72 - Only 20.1% of students took a major course in their first year. Taking major coursework in the first year led to a 29.3% point increase in award-earning. 55.6% of students with first-year major coursework earned an award. That number is up from 26.3% for students without first-year major coursework.\nMATH 30 (n:19), CHEM 1A (n:17), and CHEM 3A (n:17) are among the most popular major courses attempted in the first year, though the number of students taking these courses is relatively small. ENGL 1A \u0026amp; 1B (n:104 \u0026amp; 35, resp), PSYC 100 (n: 50), and MATH 12 (n:32) are the most popular general education courses attempted in the first year.\nEarly Signs: A Difference in First-Year Courses by Award-Earners #Award-earners had a higher proportion of enrollments in MATH 30, CHEM 1A, and CHEM 3A, along with GE courses PSYC 0105, and COMM 0003 during their first year. Award earners were less likely to attempt ENGL 0000N or MATH 0000A.\nResilient Courses #First Year Math and Chemistry or Bust\nEnrollment in MATH 0030 during the first year, regardless of outcome, is one of the best predictors for success, 85.0% of enrollments ultimately lead to an award.\nRegardless of course outcome, 72.0% (n:75) of first-year enrollments in CHEM 0001A, CHEM 0003A, ENGL 0001C, or MATH 0030 directly contributed to an award or transfer. First-year enrollments in MATH 0013, MATH 0027, and/or MATH 0029 were also very good indicators that the student is on track for completion. 53.2% (n:62) of enrollments in those courses led to an award.\nPreparedness plays it part. These courses tend to have higher than average course success rates: All ENGL 0001C (n:16) passed, along with 76.5% (n:17) in CHEM 1A, 72.7% (n:11) in CHEM 3A, and 60.0% (n:20) in MATH 0030. These students also tend to be relatively tenacious: Two out of the three CHEM 0003A students, and 75.0% (n:8) of the MATH 0030 students who did not pass their first course attempts still completed their award or transfer goal.\n**Follow Up: ** Course availability likely plays a key role as well. Preliminary findings from a concurrent study on entry-level chemistry courses (i.e. CHEM 1A, 2A, and 3A) show that the 2017-18 cohort tend to complete CHEM 3A or CHEM 1A in their second year. Priority registration placement may be playing a critical role, as the new incoming cohort tend to have later registration dates in their first year than continuing students.\nB\u0026rsquo;s Aren\u0026rsquo;t Leading to Degrees #\rFirst-Year Enrollments: Grade Distribution\rA B C D F W Grade Frequency 216 126 95 31 85 82 Proportion of Grades 34.0% 19.8% 15.0% 4.9% 13.4% 12.9% Grade Led to Award 62.5% 41.3% 27.4% 29.0% 11.8% 15.9% * Ten P/NP Grades Omitted\rOf 645 enrollments, 419 were a B or lower. In general, enrollments earning a B, C, D, F, or W led to an award only 24.1% of the time. An A grade leads to an award 60.6% of the time. The proportion of enrollments leading to awards drops to 38.9% for a B grade. These figures can be seen in detail in the table below.\nAward-earning shifts significantly if the B was earned in one or more of the significant courses noted in the last section.: A B in CHEM 0001A, CHEM 0003A, ENGL 0001C, and/or MATH 0030 improves award earning to 71.1% (n:45). A B in ENGL 0001B, HIST 0017A, MATH 0013, MATH 0027, MATH 0029, and/or NUTF 0010 also improve award-earning to 57.1% (n:88).\nOn the other side, the list of courses that do not lead to an award is substantial. Only 12.2% (n:286) of enrollments earning B or lower in this group lead to an award: ANTH 0001, ANTH 0002, ARHI 0101, CHEM 0000A, COMM 0001, ENGL 0000N, ENGL 0001A, HDEV 0001, HED 0002, HIST 0017B, MATH 0000A, MATH 0000D, MATH 0012,MUS 0002, PSYC 0100, SOC 0001,SPAN 0001. These courses make up a two-thirds, 429 of 645, of the cohort\u0026rsquo;s first-year enrollments.\nMajor Coursework Trends #The figures below calculate the odds of a course enrollment\u0026rsquo;s likelihood of leading to an award, regardless of the grade earned in that class. An odds ratio (OR) of 1 is equivalent to a 50/50 chance of leading to an award. ORs higher than one indicate enrollment in that course improves the odds of earning an award. Similarly, ORs lower than one indicate enrollment in that course lessen the odds of earning an award.\nEnrollment in PHYS 0105\\L PHYS 0110\\L, BIOL 0002 or BIOL 0003 are the best indicators that the student is on track for an award. Looking back at the degree requirements and prequisite tree, this finding is relatively intuitive. Biology for Transfer students tend to tackle their Chemistry and Math requirements in their first two years, leading to the completion of Physics and Biology coursework in later years.\nStudents tend to opt for MATH 30 (OR: 0.68, n: 46) over MATH 16A (OR: 0.39, n: 12). While enrollment in either course isn\u0026rsquo;t particularly indicative of award-earning, both courses have odds ratios under 1, those who enroll in MATH 30 have a slightly better odds of earning an award or transferring.\nIn Physics, students tend to enroll in PHYS 105\\L (OR: 1.56, n:30) and 110\\L (OR: 12.5, n:20) over PHYS 205\\L (OR: 1.25, n:3). No students in this cohort (to date) have completed PHYS 210\\L. The odds for award earning with a PHYS 110/L enrollment are quite literally off the charts. This is a good indicator that PHYS coursework is some of the last work to be completed before earning an award or transferring.\nGeneral Education and Prerequisite Courses #An odds ratio of 1 is equivalent to a 50-50 probability that enrolling in a class, regardless of outcome, will lead to an award or transfer. Three-quarters of the courses have an odds ratio greater than 3 (OR \u0026gt;3), indicating that attending these courses are three times as likely to lead to an award than other overall coursework the student is taking.\nThe Math Ladder #There is a laddering effect in the data. Near the lowest \u0026ldquo;rungs\u0026rdquo; of the ladder are prep courses ENGL N, MATH A \u0026amp; D. At the highest end is the runaway leader for award-earning, MATH 31 (OR: 41.7, n:27), a prerequisite for the PHYS 200 series. Most groups appear to cluster around a Math course following a natural hierarchy:\nMATH A \u0026amp; D (ORs \u0026lt; 1), MATH 12 (OR: 1.97, n:60), MATH 27 (OR: 3.7, n:46), MATH 29 (OR: 8.23, n:23), MATH 13 (OR 10.3, n:60), before reaching MATH 31.\nMATH 13 is not a required or recommended course, yet 78.3% (n:47) of the 60 students took MATH 13 as declared Biology for Transfer majors.\nPlenty of Time to Complete General Education #Students in Biology tend to complete their general education early in their academic career. The degree\u0026rsquo;s required courses follow a hierarchical design (i.e. CHEM 1A must precede BIOL 1, which precedes BIOL 2 \u0026amp; 3, etc). The degree structure may also give a partial explanation as to why the overall odds for award earning by general education and prep courses is so high. Students are taking longer to complete their required coursework, giving them plenty of time to complete their general education.\nMuch of the Academic Plan is open to the student.\nSuccessful students tend to meet:\nIGETC 3A|B: Arts and Humanities with MUS 2 (OR: 6.67, n:43), ARHI 101 (OR: 5.79, n:30), and HIST 17B (OR: 6.45, n: 49);\nIGETC 4: Social and Behavioral Sciences with SOC 0001 (OR: 6.67, n: 43) and PSYC 100 (OR: 4.14, n: 74); and\nIGETC 6: Language Other Than English with SPAN 1 (OR: 8.33, n: 22).\nOnly IGETC 4 has explicit recommendations on the Academic Plan.\nContrary to the plan, students tend to forgo IGETC 5A|B Physical and Biological Sciences courses ESS 1 and GEOG 3 as recommended in the Academic Plan. These courses are not included in Figure 7 due to low enrollment. IGETC 5A|B is typically met with the major\u0026rsquo;s required coursework. Similarly, students who attempt ANTH 1 (OR: 0.10, n:22) tend to have the lowest odds of completion in the cohort. Enrollment in ANTH 1 as a Biology for Transfer major may be a good indicator the student is off-track.\n","date":"18 November 2021","permalink":"/docs/2021-11-18-barriers-in-biology/barriers/","section":"Docs","summary":"This study tracks the impact of course-taking and change-in-major habits on award-earning and transfers for the 224 Biology for Transfer majors in the 2017-18 cohort.","title":"Barriers in Biology"},{"content":"","date":null,"permalink":"/tags/ir/","section":"Tags","summary":"","title":"IR"},{"content":"","date":null,"permalink":"/categories/ir/","section":"Categories","summary":"","title":"IR"},{"content":"","date":null,"permalink":"/categories/rstats/","section":"Categories","summary":"","title":"Rstats"},{"content":"","date":null,"permalink":"/categories/statistics/","section":"Categories","summary":"","title":"Statistics"},{"content":"","date":null,"permalink":"/categories/visualizations/","section":"Categories","summary":"","title":"Visualizations"},{"content":"","date":null,"permalink":"/tags/classification/","section":"Tags","summary":"","title":"Classification"},{"content":"","date":null,"permalink":"/categories/classification/","section":"Categories","summary":"","title":"Classification"},{"content":" Abstract\nFrequently, k-nearest neighbors classification is applied with features chosen arbitrarily, while `k` is adjusted to improve the accuracy of the model. For this experiment, `k` is fixed. Using correlations with the strict acknowledgement that all features are continuous, a feature set with high correlation within itself is selection. Visual analysis using density and scatter plots show that these features also share distinct distributions and thusly make great candidates for k-nearest neighbors clustering. The four features return a well classified set, while tweaking the set by removing features with less distinctive groupings returns very high accuracy with an increased risk of overfit. Both features sets perform better than using all features with the same fixed hyperparameter. Introduction #With a small database, it\u0026rsquo;s easy enough to fit all features to a model, or select all quantitative data, then frequently modify the hyperparameters to boost up the accuracy. But what information are we losing in the process?\nFeature selection is just as important as hyperparameter choice, yet many classifiers cherry-pick features without much thought for their empirical basis. The focus of this experiment will look at the impact of feature selection on wheat kernel classification. Given the distinctive groupings, that occur within many features, k-nearest neighbors will be used for classification. While the hyperparameter, k will be easier to tweak, we must account for the impact of boosting hyperparameters on the risk of overfit.\nData Information and Attributes #Data Source #Wheat seed data provided by UCI’s Center for Machine Learning and Intelligent Systems.\nhttps://archive.ics.uci.edu/ml/datasets/seeds#\rData Information and Attributes #There are 210 sample kernels of three varieties of wheat, Kama, Rosa, and Canadian.\nEach wheat kernel is assigned seven attributes: grain area, perimeter, compactness, length and width of kernel, asymmetry coefficient and length of kernel groove.\nEach measurement is given in millimeters.\nData Structure and Cleaning #Our source data is a text file with headless, tab-separated data (Table 1).\nTable 1: Raw Data Input\rV1\rV2\rV3\rV4\rV5\rV6\rV7\rV8\r15.26\r14.84\r0.8710\r5.763\r3.312\r2.221\r5.220\r1\r14.88\r14.57\r0.8811\r5.554\r3.333\r1.018\r4.956\r1\r14.29\r14.09\r0.9050\r5.291\r3.337\r2.699\r4.825\r1\rTo improve readability and analysis, each feature is assigned a descriptive name. The response variable, ‘wheat‘, is factored into a categorical variable with levels \u0026ldquo;Kama\u0026rdquo;, \u0026ldquo;Rosa\u0026rdquo;, and \u0026ldquo;Canadian\u0026rdquo; replacing numeric variables 1,2, and 3, respectively (Table 2).\nTable 2: Legible Dataframe\rarea\rperimeter\rcompactness\rlength\rwidth\rasymmetry\rgroove\rwheat\r15.26\r14.84\r0.8710\r5.763\r3.312\r2.221\r5.220\rKama\r14.88\r14.57\r0.8811\r5.554\r3.333\r1.018\r4.956\rKama\r14.29\r14.09\r0.9050\r5.291\r3.337\r2.699\r4.825\rKama\r13.84\r13.94\r0.8955\r5.324\r3.379\r2.259\r4.805\rKama\r16.14\r14.99\r0.9034\r5.658\r3.562\r1.355\r5.175\rKama\r14.38\r14.21\r0.8951\r5.386\r3.312\r2.462\r4.956\rKama\rMissing Values #Upon initial inspection, the data is complete and contains no missing values.\n{: .align-center}\nFigure 1. Outlier and Quantile Analysis by Feature Two or three Kama kernel appears to fall outside the normal distribution in several categorical factors (Fig 1), but they are not far enough away from the rest of the level to warrant removal. With no major outliers and no missing values, data cleaning was a relatively simple process involving only the adjustment of variate names and factor levels.\nVisualizations and Analysis #The wheats create distinctive groups when classified by area and perimeter. Length and width also provide significant groupings, while compactness, asymmetry, and groove have substantial overlap between the wheat variants (Fig 2).\nFigure 2: Histograms with Density Overlays by Feature\nWhen classifying by wheat groove, in particular, the wheats fall into two distinctive groups, with Canadian and Kama wheats sharing nearly identical distributions. The groove on Rosa wheat is significantly longer and has almost no overlap with the opposing groups.\nCorrelation and Associativity #With no categorical data, we can create a table of comparative values between all of our variates (Fig 3).\nFigure 3: Correlation and Associativity Area is highly correlated with the perimeter (r = 0.994), length (r=0.950), and width (r=0.971) of a given kernel. These values are highly correlated with each other as well. Another notable factor, the kernel groove, is also highly correlated with the four noted factors.\nClassification #Given the distinctions in distribution, as well as the high correlation between the given factors, k-nearest neighbors with factors area, perimeter, length, and width as the classifying features. All features are given in millimeters and will not be standardized.\nAll experiments are completed in R with class::knn, k=5. The train-test ratio will be set at 80/20 to provide an adequate test sample size, n=42. Training set, n=168.\nK-Nearest Neighbors with Selected Features #Table 3: Confusion Matrix: Classification with Area, Perimeter, Height and Width\rPred/Acc Kama\rRosa\rCanadian\rKama\r11\r1\r0\rRosa\r1\r14\r0\rCanadian\r2\r0\r13\rThe initial classification did well, 90.47% (Table 3). Two issues stand out: (a) Kama kernels are misclassified relatively frequently, and (b) other kernels are misclassified as Kama kernels relatively frequently. These results are relatively intuitive, as the peak of the distribution for Kama kernels falls between Rosa and Canadian kernels for the factors area, perimeter, length, and width.\nThe Kama distribution tails for length and width, as noted before overlap significantly. There is very little to no misclassification between Rosa and Canadian wheat kernels as the factor distributions and means are consistently more distant from each other when compared to the difference in means with Kama kernels for either wheat.\nK-Nearest Neighbors with Minimal Features #Table 4: Confusion Matrix: Classification with Area and Perimeter\rPred/Acc Kama\rRosa\rCanadian\rKama\r13\r1\r0\rRosa\r0\r14\r0\rCanadian\r1\r0\r13\rRemoving height and width as factors increased the accuracy of our knn prediction to 95.24%, but that number on it’s own raises some suspicion (Table 4). The highly accurate result is likely more accidental, but it does show us that these features with more overlap in the tails do have a significant impact on the number of misclassified values.\nK-Nearest Neighbors with All Features #Table 5: Confusion Matrix: Classification with All Features\rPred/Acc\rKama\rRosa\rCanadian\rKama\r11\r1\r0\rRosa\r2\r14\r0\rCanadian\r1\r0\r13\rAs a control, we can train with all seven features. Given the significant overlap in compactness, asymmetry, and groove, the accuracy is expected to be lower than when strictly training with area and perimeter. The results are as expected, 90.47% (Table 5).\nConclusion #The experiment shows the value of data exploration when choosing features. The experiment also shows that increasing the number of features doesn’t necessarily improve the accuracy of testing. Taking care to minimize inadvertent p-hacking should always be at the forefront of a researcher’s mind when building experiments.\nWith that said, the analysis is based off of a single fixed training and test sample. Improvements can be made to the experiment by replicating the analysis with cross validation, bootstrapping, or k-folds to ensure replicability.\n","date":"17 May 2020","permalink":"/docs/2020-05-17-feature-choice-in-classification/2020-05-17-feature-choice-in-classification/","section":"Docs","summary":"Feature selection is just as important as hyperparameter choice, yet many classifiers cherry-pick features without much empirical basis. While hyperparameters are easier to tweak, we must account for the impact of boosting these hyperparameters and the risk and impact of overfit.","title":"Feature Choice in Classification of Wheat Kernels"},{"content":"","date":null,"permalink":"/tags/gghighlight/","section":"Tags","summary":"","title":"Gghighlight"},{"content":" Side-by-Side Comparison of (a) Generic Plot and (b) Faceted Plots with Highlight\nVisualizing Categorical Data #What\u0026rsquo;s the most effective way to visualize comparisons between variable factors? How do you share a lot of information without muddying up your visualizations? There definitely isn\u0026rsquo;t one answer, but here\u0026rsquo;s one solution: faceted plots with gghighlight.\nFor sample data, I’m setting up two normal curves with offset sample means x̄ = ± 2. While we\u0026rsquo;re using two factors for simplicity, this method actual becomes more effective when the number of factors increases.1 For our sample data, Lefties are disperses around a mean left of center, while Righties collect right of center. The variances are modified as well to exaggerate the difference in factors, but we\u0026rsquo;ll get into that in a second.\n# create a sample dataset of two normal curves with given classes samples \u0026lt;- data.frame(val = rnorm(100, 2,2), pos = (\u0026#34;Righties\u0026#34;)) %\u0026gt;% rbind(data.frame(val = rnorm(100, -2,1), pos = (\u0026#34;Lefties\u0026#34;)) ) Value Position 1 3.728539 Righties 2 1.465043 Righties 101 -0.210901 Lefties 102 -2.117621 Lefties Statistics #Summary statistics as a visual annotation are helpful when determining differences in factors. We have a couple of “out of the box” options through the use of summary and the like.\n# create generic statistical summary samples %\u0026gt;% select_if(is.numeric) %\u0026gt;% map(~tidy(summary(.x))) %\u0026gt;% # compute tidy summary of each var do.call(rbind, .) -\u0026gt; stats # bind list elements into df Minimum Q1 Median Mean Q3 Maximum Value -4.915701 -2.159441 -0.8809261 -0.0567143 1.785081 6.861341 Or we can build our own custom summary table. In this case, we\u0026rsquo;re only interested in adding mean and deviation to our visuals.\n# create a custom summary, in this case, just the mean and sd summary_stats \u0026lt;- samples %\u0026gt;% group_by(pos) %\u0026gt;% summarize_at(vars(val), funs(Mean = mean, SD = sd)) Position Mean SD Righties 1.92 2.22 Lefties -2.03 0.98 Generic Plots #Generic ggplots tend to be a bit bland, which is fine in some cases. The generic plot is almost identical to R\u0026rsquo;s plot() function.2\nGeneric Histogram (w/ Bare Bones Aesthetics)\n# Create a simple histogram (w/ bare bones aesthetics) ggplot(samples, aes(x=val)) + geom_histogram(alpha= 0.7, bins = 40) + # 70% opacity, 40 slices labs(title =\u0026#34;Distribution of Lefties and Righties\u0026#34;, x =\u0026#34;Sample Value\u0026#34;, y= \u0026#34;Frequency\u0026#34;) + # All Labels theme_bw() # B\u0026amp;W Theme In our case, a generic plot shows us that the distribution is bimodal, but we really can\u0026rsquo;t determine a lot about the characteristics of either factors\u0026rsquo; distribution. We are somewhat lucky, in this case, that if we were to create a classifier based strictly off of the information above, we can see that the cutoff should be about x=0. Aside from that, it\u0026rsquo;s difficult to read exactly what\u0026rsquo;s going on.\nFaceted Plots # Histograms Faceted by Categorical Factor\nggplot(samples, aes(x=val,fill=pos)) + # Graphs geom_histogram(aes(y=..density..),binwidth=.5, alpha=.5, position=\u0026#34;identity\u0026#34;) + geom_density(alpha=.3) + # One figure per class facet_grid(.~pos, scales = \u0026#34;fixed\u0026#34;)+ # Mean line geom_vline(data = summary_stats, mapping = aes(xintercept = Mean), linetype=\u0026#34;dotted\u0026#34;, color = \u0026#34;blue\u0026#34;, size=0.7) + # Annotations geom_text(data=summary_stats, inherit.aes=FALSE, aes(vjust=\u0026#34;inward\u0026#34;, hjust = \u0026#34;inward\u0026#34;, x = Inf, y = Inf, family = \u0026#34;serif\u0026#34;, label=paste(\u0026#34;\\nMean:\u0026#34;,Mean,\u0026#34;\\nSD:\u0026#34;,SD, \u0026#34;\\n\u0026#34;))) ... Even without the summary statistics, it\u0026rsquo;s much easier to discern the differences in two groups. At this stage, I\u0026rsquo;ve introduced the summary statistics and a mean line to increase comprehension.\nWith that said, there are a handful of important features in this graphic that can be easily overlooked:\nFix the y-axis. Without a fixed axis, we have no ability to quickly discern differences in distribution Drop the opacity of the histogram. Overlapping data can get lost in the shuffle. Adding That Highlight # Histograms Faceted by Categorical Factor\nggplot(samples, aes(x=val,fill=pos)) + geom_histogram(aes(y=..density..),binwidth=.5, alpha=.5, position=\u0026#34;identity\u0026#34;) + geom_density(alpha=.3) + \u0026lt;b\u0026gt;gghighlight::gghighlight()\u0026lt;/b\u0026gt; # add gghighlight ... This is such a beautiful, yet deceptively simple trick to improving readability, a gghighlight!3 It\u0026rsquo;s worth noting that we don\u0026rsquo;t need any additional arguments as the function is highlighting the given data per facet. Put another way, gghighlight is intuitive enough to figure out what should be grayed out and what should pop. Highlighting works with more than two factors as well. Anything that isn\u0026rsquo;t the primary data simply sits in the background.\nWe\u0026rsquo;re just scraping the surface of what gghighlight can do for data viz. It\u0026rsquo;s incredibly effective for singling out notable traits in a class, emphasizing trends, or simply comparing a subset of data to the rest of a sample.\nGGPlot Summary #There are a lot of bits and pieces to these graphs, here\u0026rsquo;s a quick recap on what role each piece plays.\ngeom_histogram: creates histogram, plotting feature values against their frequencies\ny=..density.. shifts the y-axis to a relative frequency\nbin_width: the number of “slices” that we cut our data into. Play with this number! Can be relative, 0-1, or a number of slices (i.e. binwidth = 10 for ten equal sized bins)\nalpha: the transparency of our data, from fully transparent (0.0) to opaque (1.0)\nposition: relationship between the classes, can be “identity”, “stack”, “dodge”. Using facet_grid, “identity” has the added benefit of creating equal axes for each figure.\ngeom_density: smooths the histogram into a density plot, with y-axis as the relative frequency for a point\ngeom_vline: vertical line, in this case used to create a mean line\ngghighlight: great way to show our data compared to the relative spread. Creates a “shadow” of the overall distribution.\nfacet_grid: breaks visuals into individual figures by feature.\nThe researcher that inspired this post needed to present characteristics for six separate factors in a concise manner.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI\u0026rsquo;m oversimplifying a bit. This plot isn\u0026rsquo;t completely generic. Titles, transparency, and theming aren\u0026rsquo;t necessarily something you would get with base plot().\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRDocumentation: gghighlight.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"12 May 2020","permalink":"/docs/2020-05-12-facet-figures/2020-05-12-facet-figures/","section":"Docs","summary":"Using \u003ccode\u003egghighlight()\u003c/code\u003e to effectively visualize comparisons between factors","title":"Highlights and Facets for Visualizing Categorical Data"},{"content":" I\u0026rsquo;ll likely be revisiting this project in the future to see how my analysis has changed. With that said, I\u0026rsquo;m presenting this project as I did in 2016. So much has changed in the last four years. I\u0026rsquo;m excited to rework this project and pull in some fresh data. It should make for a very interesting before-and-after.\nFinal product, top left, with the three layers used for analysis and composition\r{% include gallery id=\u0026ldquo;gallery\u0026rdquo; caption=\u0026ldquo;Paper Map Examples at Regional, District and City Levels\u0026rdquo; %}\nGrassroots Organization for Presidental Primary Campaigning • Spring 2016\nWashoe County Canvassing, Feb 2016\nProject Idea and Timeline #Create a canvass map to help direct campaign efforts, with a focus on targeting likely voters in Northern California. Ideally this will be a dynamic project, updated as precincts are contacted.\nPrimary Data #American Community Survey 5‐Year Estimates — TIGER/Line1\nStatewide Database – Hosted by University of California, Berkeley 2\nTimeline # Collect and sort 2008/2012 elections data and precinct information Find hotspots, Precincts with higher turnouts and moderate to high Democrat preference, to canvass first. Most valuable precincts will have a moderate to high population of registered voters and good election turnouts. Compile map, determine most effective map style Compile statistical analysis Develop an interface to display precincts with canvassed and not canvassed information May be best done through a web app Project Summary #The intent of this project is to determine the best voting precincts, within the state of California, in which to send political canvassers. Priority will be based on favorable 2012 General Election and Registration demographics, as well as 2014 Census demographics. The map will be best utilized by a small team or individual campaign coordinator who will then send teams out to the highest priority precincts.\nPurpose #It’s a given fault that canvassing efforts generally cannot reach all of the neighborhoods that they want to. So how do you prioritize which neighborhoods to visit and which get the inevitable pass?\nI’ve established three criteria:\nHigh density areas are a big target. High density areas, in the context of this project, will be defined in two different ways.\nPrecincts with the highest rate of Democrat and “No Party Preference (NPP)” counts.\nAreas with overall high population densities as well, since voters can register or change their affiliation up until May 23rd, 2016.\nIt’s important to know the difference between high population densities and high voter registration densities. It’s not enough to know how many people can vote. It’s more important, at this stage, to focus on confirmed registered voter numbers. With that said,\nTurnout, the quantity of registered voters that actually voted in past elections, will weigh in on a precinct’s canvass priority. In a hypothetical situation, a high turnout in a lower population precinct may prove more valuable than a moderately low turnout in a higher population area. Giving each value an equal weight will allow us to quantitatively determine where to send our canvassers. Process #Data Research #Voter Turnout and Registered Voter Data\nOur state is broken up into over 21,000 voter precincts, which may also be referred to as election districts. Each precinct divides a city or census designated place to distribute the population among relatively equal populations.\nElection results and voter demographic information can be linked to precinct shapefiles.\n2008 v. 2012\nThe differences in voting demographics between 2008 and 2012 is not substantial, so emphasis is placed on relevance. The more recent information should prove more accurate and influential than any trends that occurred with the higher turnout of 2008.\nPopulation Density\nLikewise, Census information via the TIGER database can be used to collect general population density information. The detailed Census demographic and population information, linked to the land area of the state’s census shapefiles, can provide us with the population density. Creating a population density graphic will be most accurate at the block group level (block level unavailable).\nData Sources #Statewide Database – Hosted by University of California, Berkeley2\n2012 State Precincts Shapefile 2012 Voter Turnout Demographics Table (.dbf) 2012 Voter Registration Demographics Table (.dbf) American Community Survey 5-Year Estimates — TIGER/Line1\n2014 California Block Group Shapefile 2014 5‐Year Estimate “X01 Age and Sex” Demographics Table (.dbf) TIGER/Line Shapefile – California Primary and Secondary Roads, State County Shapefiles USGS Water Resources – California Lakes Natural Earth Data – Oceans\nData Management and Joins #Before joining the data, some adjustments had to be made to make the information more palatable for mapping. Joins were made between the Precinct shapefile and both the voter turnout table and registration table. The Census block groups and demographic table were joined based on the block group ID number. The newly joined tables and shapefiles were then exported as three shapefiles into the master geodatabase.\nNull Values\nThe precinct data tables included areas referred to as “unassigned”. For voter turnout this meant that the geographic area had no voters turn out. Likewise for registration, unassigned areas did not have any registered voters.\nTo aid with the conversion from vector to raster, as well as visualization with the vector data itself, several custom tables were created to reduce null value and divide‐by‐zero errors. Null values were converted to zero.\nA pre‐logic script code was then added to several new custom fields eliminate the divide‐by‐zero errors. The custom field details are noted in the next section. This is a snippet of VB Script used to divert values to zero in locations where there are no registered voters. If the number of registered voters is greater than zero, the calculation will run as expected.\n​``` If [TOTREG] = 0 Then val = 0 Else val = [TOTVAL]/[TOTREG] End If ​``` Custom Fields and Field Calculations\nThree custom fields were created for this project:\nTurnout Rate Target Voters Census Density and Demographic Turnout Rate – Calculate the percentage of registered voters that turned out to vote. Divide the number of vote by the number of registered voters accounted for.\n[Turnout] = [TOTREG]/[TOTVOTE]\nTarget Voters – Calculate the percentage of total registered voters that have declared themselves as Democrats or have declined to state a party.\n[TV_Vote] = ([DEM]+[DCL])/[TOTREG_R] Target Vote = (Democrat + Decline to State)/Total Registered Voters Census Density with Target Demographic – a) Remove all children from each block group’s total counts, then, b) divide the adult count by the land area of their respective block group\n[Voting_Age] = [B00001e1] ‐ ([B01001e3]‐[B01001e4]‐[ B01001e5]‐[B01001e27]‐[B01001e28]‐[ B01001e29]) Voting Age = Total Count – (“Male \u003c5” – “Male 5‐9” – “Male 10‐14” – “Female \u003c5” – “Female 5‐9” – “Female 10‐14”)\n[Census_Density] = [Voting_Age]/[ALAND] Block Group Density = Voting Age Adults/Block Group Land Area (sq. meters)\nModifications to Jenks\n(a) Jenks on Target Voters Layer and (b) Turnout Raster All vector maps were symbolized using the Jenks method with four breaks (five sections).\nIn order to adequately show the difference between areas with no and low respective rates, the Jenks scale was modified to show six sections instead of five. The sixth section is simply zero values.\nA section of the target voters map is above, (a). The white stripes over what is effectively Folsom Lake are areas with no registered voters. This contrasts the low Democrat and Decline to State (NPP) rates in the Lake’s surrounding neighborhoods, shown in red. The blue symbology in the northeast corner is an area of substantially higher Democrat/NPP rates.\nPolygon to Raster Conversion #Using the three newly created fields outlined in the Custom Fields and Field Calculations section, each of the three main shapefiles were converted to raster using the Polygon to Raster tool. Below is the conversion for the Turnout_Rate shapefile to raster. This process creates three new rasters:\nTurnout_Rate_Raster, Target_Voter_Raster, and Census_Density_Raster. The new rasters were added to the master geodatabase upon completion. A snippet of the turnout rate near Folsom Lake is shown above, (b). Areas of dark green have the highest turnouts. The scale moves through the light greens to yellows, oranges, then to bright red denoting zero turnout.\nRaster Math #\r(a) The Multiplier and (b) Multiplier Raster with Precinct Poly Overlay Using the raster calculator, the three raster layers are multiplied together to produce a new raster, The_Multiplier. By multiplying the raster layers together, equal weight is given to the turnout, target voters and population densities of a specific area. Precinct boundaries and census block group boundaries are completely disregarded in this step.\nAbove, (a), is Folsom Lake and it’s surrounding areas, primarily to the south and west. As we’ve seen with the vector and raster maps previously, the Lake holds a very low priority value with regard to canvassing. To the south, however, is the northern section of the City of Folsom. City of Folsom shows off it’s highest priority areas, in dark green, to it’s lowest, deep red. Gray areas hold no significant population, near zero election turnouts, or near zero Democrat/NPP numbers and are considered the lowest priority.\nZonal Statistics #With the now blended Multiplier raster created, the information needs to be related back to the precinct shape files. The image, (b), shows the “blended” raster layer with the precinct overlaid. It’s clear that several precincts are not well defined. This next step will resolve that.\nThe Zonal Statistics as Table tool, below, takes the mean of each of the Multiplier raster layer within each precinct boundary. The mean information is then added to a newly created table (.dbf)\nThe new table, Multiplier_Table, is joined to the Precinct Boundary shapefile. The data is symbolized by the mean value the raster cells within each precinct.\nBy using the Multiplier raster and table as an intermediary, we’ve effectively converted our original census and precinct vector data into a dataset that fits neatly within the precinct boundaries.\nPrecincts now prioritized based on voter turnout, registration demographic, and population density Finishing Touches #\rCongressional District 13 (a) without and (b) with Highways, Hydrology and Cities To improve the map’s readability, recognizable features like highways, lakes and major city labels have been added to the map.\nThis map is heavily based on voting precinct boundaries and Congressional District boundaries. These are two features that may not be inherently recognizable to the reader. More familiar boundaries (e.g. county lines, hydrologic features) have been added to improve readability.\nProblems #Data Sources #Precinct Information\nCalifornia, more specifically the Secretary of State, does not have an aggregated list of general election votes and voter registrations by precinct available to the public. As a result, the 2012 demographic and voter information is sourced through a third party. The Berkeley School of Law information has aggregated datasets from the Statements of Vote (SOV) and Statements of Registration from California’s 58 counties, as collected by each county’s respective County Registrar of Voters or County Clerks.\nCensus Information\nThere is a modest learning curve that comes with the readability of Census data. With some research, there are resources available to decode file names, field names and field descriptions. Census data is also laden with concatenations and acronyms. Fortunately, the Census has made documentation widely available online.\nVariance in Population #\rCongressional District 1, Lowest Density CD Still Has A\rWell Represented City – Chico, CA\rThe initial intent of this project was to create a canvass map for Northern California. Unfortunately, the state has a delegate distribution process that would make it very difficult to focus on the northern part of the state alone.\nWith six delegates assigned to each Congressional District, then 105 delegates at‐large distributed after the primary election, it was increasingly apparent that the population heavy Los Angeles precincts should be included in dataset. The decision was made to prevent any major skewing of the prioritization process. It also had the added benefit of providing the same spatial analysis to what potentially could be the most delegate rich section of the state.\nCongressional District 12 – San Francisco\rLargely Overrepresented\rCreating a scale that allowed for at least one high priority precinct in each Congressional District was essential. The statistical over-representation of more populated areas was considered acceptable, as they will ultimately have the largest influence on at‐large delegates.\nProduction #Web Map, Limited Congressional Districts ‐ http://arcg.is/1TnmgNs Link out of date\nI developed this project to challenge myself. Many aspects of this project were brand new to me and it was an absolute thrill to be able to troubleshoot my way through it.\nThe data research proved more challenging than I expected, but good data is the literal cornerstone of a good project. Utilizing two completely different data sources, finding the shapefiles, inspecting the data tables then ensuring my joins would be adequate, took up the first third of my semester.\nCreating a formula that answered the question, “What gives a precinct a higher priority over another?” proved to be the largest challenge. My answer: a top priority precinct will have a moderate to high population density, high voter turnout in the 2012 General Election, and a high rate of Democrat and unaffiliated voters. These are the best areas to send an individual out into the field to canvass and expect not only a good number of contacts made, but fair majority of positive interactions.\nConverting data from vector to raster is old hat. But how do you convert a custom raster layer to fit the polygons of an established vector file? What would the data quality be like? What I loved about this project was the opportunity to dive right into the spatial analysis tools like the raster calculator and zonal statistics. This project’s two major shapefiles, 2012 Election Precincts and 2014 Census Block Groups, were not compatible with regard to sharing information back‐and‐forth. With the data converted to raster, I had a whole new picture to look at.\nRaster math allowed me to set equal rates for each of my three criteria, zero to one, then give each rate equal weight by multiplying the three rates together. What came next was all new. I have a raster map that no longer fits the shapes I started with. The Zonal Statistics toolset was an absolute treat to work with. Creating the statistics table based on the raster cells, limited to the precinct shapefile boundaries, was the most novel part of this project for me.\nAn unexpected result of this project was the depth in which I was able to work with the raster data. Previous projects made rasters feel heavy and unnecessary. This project allowed me to manipulate data to the extent I needed to, then convert it back to vector data with the intent of using the shapefiles in an online environment. Had I not planned to create a web map, the raster data may have proven robust enough for its own project.\nhttps://www.census.gov/geo/maps-data/data/tiger-data.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttp://statewidedatabase.org/d10/index.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"18 April 2020","permalink":"/docs/2020-04-18-california-canvass-map/2020-04-18-california-canvass-map/","section":"Docs","summary":"The intent of this project is to determine the best voting precincts, within the state of California, in which to send political canvassers. The map will be best utilized by a small team or individual campaign coordinator who will then send teams out to the highest priority precincts.","title":"California Canvass Map"},{"content":"","date":null,"permalink":"/tags/gis/","section":"Tags","summary":"","title":"GIS"},{"content":"","date":null,"permalink":"/categories/gis/","section":"Categories","summary":"","title":"GIS"},{"content":"","date":null,"permalink":"/tags/spatial-analysis/","section":"Tags","summary":"","title":"Spatial Analysis"},{"content":"","date":null,"permalink":"/tags/statistics/","section":"Tags","summary":"","title":"Statistics"},{"content":"","date":null,"permalink":"/tags/volunteer-work/","section":"Tags","summary":"","title":"Volunteer Work"},{"content":"","date":null,"permalink":"/tags/git/","section":"Tags","summary":"","title":"Git"},{"content":"","date":null,"permalink":"/categories/git/","section":"Categories","summary":"","title":"Git"},{"content":" The original post is a response to a class forum. This post is just a handy way to determine how the site will handle markdown language while also providing some interesting content.\nThis was originally part of a response to #151, but it should be good information for anyone trying to troubleshoot git version control. I\u0026rsquo;ve pulled some of the specific troubleshooting out and added a bunch of detail on the common commands. My hope is that it gives you guys a kind of understanding of what git is doing in the background.\nPleeease be careful about just blindly typing in git commands. As safe as git version control is, you run a real risk of overwriting your files by just trying to pull simply when git asks you to.\nShort version: #git clone url: Asking the remote (online) repository to make a local (PC) copy\ngit pull: Including or \u0026ldquo;pulling down\u0026rdquo; changes from the remote repository to your local\ngit status: Looking at the differences between the remote and local. Awesome tool for troubleshooting.\ngit diff: Shows changes between the remote file and the local\ngit add: Prep stage getting ready to add new files that are local but you want to be remote. This is also where git starts tracking files if you\u0026rsquo;ve heard that term before.\ngit commit: Takes the files to be updated to the remote repo, creates a \u0026ldquo;timestamp of sorts\u0026rdquo; and stages them to be included in the remote repo, i.e. you\u0026rsquo;re committing the changes you\u0026rsquo;ve made up to a certain point. For us, this is usually just one commit (when we complete the file).\ngit push: Updates the remote repository\nA bit more in-depth #Git Pull So let\u0026rsquo;s talk real quick about what git pull does, especially considering if you\u0026rsquo;re pushing your changes you likely have a complete project that shouldn\u0026rsquo;t be altered much. git pull is pulling remote (online) changes down to your local (PC) repository. Disclaimer, I use Git Bash console for my commands, so RStudio will be slightly different, but the commands are the same. For this class, we don\u0026rsquo;t really pull assignments as much since we\u0026rsquo;re simply using git clone url to create a local copy. git pull is more effective in cases like #80 where there\u0026rsquo;s been a revision.[For revisions see : #153] Assuming you\u0026rsquo;ve already cloned the file, git pull will \u0026ldquo;pull down\u0026rdquo; any changes made to the remote repository and include them in your local file.\nGit Status and Diff But you\u0026rsquo;ve been working on your files before the changes were made remotely, right? Now you have a high risk of merge conflicts. Try out git status to see what files have changed.\nMine looks like this:\n$ git status\rOn branch master\rYour branch is up-to-date with \u0026#39;origin/master\u0026#39;. # where your conflict is Changes not staged for commit:\r(use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to update what will be committed)\r(use \u0026#34;git checkout -- \u0026lt;file\u0026gt;...\u0026#34; to discard changes in working directory)\rmodified: q1.Rmd\rmodified: q2.Rmd\rmodified: q3.Rmd\rUntracked files:\r(use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to include in what will be committed)\r.RData\r.Rhistory\rq1.html\rq2.html\rno changes added to commit (use \u0026#34;git add\u0026#34; and/or \u0026#34;git commit -a\u0026#34;) The first few lines are telling me that I\u0026rsquo;m on the main branch (we won\u0026rsquo;t talk about branches much here, so this won\u0026rsquo;t change for the time being) and that my (local) branch, or copy of the repo, is up-to-date with the remote repository, called \u0026lsquo;origin/master\u0026rsquo;.\nAfter that we start to see file status changes. I\u0026rsquo;m currently working on q3.Rmd, and haven\u0026rsquo;t pushed any changes yet, so it\u0026rsquo;s intuitive that my modified files would be the three *.Rmd files.\nIf for some reason status tells me that I'm x commits behind masterI can look at the difference between my repo and the remote repo line-by-line withgit diff`.\n$ git diff diff --git a/q1.Rmd b/q1.Rmd index 5b76ee4..7dfe81a 100644 #commit references\r--- a/q1.Rmd # remote file\r+++ b/q1.Rmd # local file\r@@ -1,5 +1,6 @@ # lines changes\r---\rtitle: \u0026#34;Question 1\u0026#34;\r+author: \u0026#34;Jessica LaCourse\u0026#34;\r---\r@@ -15,32 +16,75 @@ The `walk` function is very similar to the `map` function except that it doesn\u0026#39;t Here, a/q1.Rmd b/q1.Rmd are my remote and local files being compared, respectively. Values between the @ symbols are line references and the + is a line, an author tag at the start of assignment 1, I added to the start of the file. Intuitively, you can also have - changes for lines deleted. git diff is especially handy when you are working in a team and the remote repo is changing while you\u0026rsquo;re working locally.\nGit Add, Commit and Push The local repository doesn\u0026rsquo;t have a copy of q1.html or q2.html so I\u0026rsquo;ll tell bash \u0026ldquo;Hey, when I push, I want to push these files, too\u0026rdquo; with the command git add q1.html q2.html (and ultimately q3 as well).\nI\u0026rsquo;ve added my files, now I want to commit them, git commit -m \u0026quot;Complete Assignments\u0026quot;, where -m is called a flag saying \u0026ldquo;I want to include the message, \u0026lsquo;Complete assignment\u0026rsquo;\u0026rdquo;.\nFrom there, I can push my changes, git push origin master.\nOnce I push my changes, all five files (three *.Rmds and two *.htmls will push to the local repository.\nSo why are git add, git commit, and git push three different commands? It could be one right?\nThe first, I hope it\u0026rsquo;s a bit intuitive as to why git add is it\u0026rsquo;s own command. You don\u0026rsquo;t necessarily want to add ALL the files in a folder to the remote repo. There is a command, use it wisely, to add all new files git add . Similarly, to add all changes, added and deleted files, etc. There\u0026rsquo;s git add -A where dot and -A respectively are flags for \u0026ldquo;all new\u0026rdquo; and \u0026ldquo;all\u0026rdquo; changes.\nI\u0026rsquo;m not the only one whose committed a file, gotten the message that it was successful, then\u0026hellip; it\u0026rsquo;s not there. It\u0026rsquo;s not where it\u0026rsquo;s supposed to be! Why isn\u0026rsquo;t my file online? There\u0026rsquo;s some nuance in the difference between git commit and git push. The way I think of it is that git commit is kind of like putting a bookmark in my book, while git push is talking about the latest changes (between this time and the last time I dropped a bookmark in my book) with a(n obviously Zoom) book club or something larger than myself. Does that make sense? commit logs changes locally, while push updates our remote repository with our changes.\nFor this class, we only commit and push once, usually. But when you\u0026rsquo;re working on a larger project you can absolutely commit and push to a repo dozens or hundreds of times.\nEdit: Formatting, added git diff information\n","date":"16 April 2020","permalink":"/docs/2020-04-16-git-commands/2020-04-16-git-commands/","section":"Docs","summary":"Good information for anyone trying to troubleshoot git version control","title":"Git Commands"},{"content":"I\u0026rsquo;m excited. It\u0026rsquo;s been a long road trying to find the right platform to build on. Fortunately, hosting on GitHub Pages, along with the incredible development of the Jekyll-based Minimal Mistakes theme has made web-development a (relatively) seamless task for integrating my projects from GitHub. I\u0026rsquo;ll post source code and repositories as reasonable. Some of the projects are University-based and don\u0026rsquo;t change much year-to-year. In those cases, sorry kids. Check out the end result but we can\u0026rsquo;t make it that easy for you.\nThe last ten or so years have been filled with amazing experiences and collaborations. Why not showcase my work and make it easier to collaborate with those that share my interests? My projects get pretty diverse: spatial models and highly detailed analytical maps, integration of bioinformation and APIs, and sociological studies are the first to come to mind. And while the ability to show off some diversity is great, it\u0026rsquo;s just a jumbled mess if it\u0026rsquo;s not well organized.\nThis site will help organize those thoughts and projects. Blog posts, much like this one, will be rare and the presentation of projects and information will take center stage. Already in the works is a series on plain-language statistics and the conversion of existing projects into a more web-friendly format. Projects and posts will have searchable tags at the bottom of each page, which should allow you to find related materials. I\u0026rsquo;ll leave a small number of static pages at the top of each page with some of my background information as well.\n","date":"1 April 2020","permalink":"/docs/2020-04-01-motivation-and-organization/2020-04-01-motivation-and-organization/","section":"Docs","summary":"I\u0026rsquo;m excited.","title":"Motivation and Organization"},{"content":"","date":null,"permalink":"/tags/about/","section":"Tags","summary":"","title":"About"},{"content":"","date":null,"permalink":"/categories/about/","section":"Categories","summary":"","title":"About"},{"content":" If quarantine brought me one gift, my once three-hour commute became three hours each day to build a portfolio and digress on some of the more interesting topics in statistics. This of course, comes at the expense of turning the dining room into my new office.\nHi, I\u0026rsquo;m Jes.\nI\u0026rsquo;m an institutional researcher with a background in Statistics and Data Science. In the office, my focus is on student equity through the use of data. I also love integrating Geographic Information Systems (GIS) as a visual and analytic aid in my work. Prior to institutional research, my work primarily focused on financial accounting and business administration. Outside of work, my projects are community-oriented and focus on voter advocacy and registration, community organization, and environmental awareness.\nBeautiful, Ethical Data #My interest in statistics started after noticing a lot of confusion in how information was being shared. Projects, like annual reports made for the public, were often unclear. Ambiguity shows up in two forms: unethical data manipulation and poor visualization.\nThere seems to be a general misunderstanding that science can tell data what to say. Answers are drafted before our models were ever designed. \u0026ldquo;Have the data show\u0026hellip;\u0026rdquo;\nIt\u0026rsquo;s true that a single dataset can tell many different stories. Small shifts in how data are accessed, filtered, and modeled can change a narrative. This fact, however, can and will be problematic. We as statisticians minimize ambiguity through transparency in our choice of practices.\nAmbiguity is also what moves statistics away from pure mathematics. It\u0026rsquo;s an applied field that borrows from both art and science. These fields are naturally complementary. Your presentation of data is useless if it lacks clarity. Simplifying visuals and creating some consistency are simple tactics to improve visualization.\nSimple mistakes and malicious manipulation alike can alter data into something unrecognizable. Having the tools and network to call out those transgressions is the first step, whereas my focus comes in teaching folks to see the issues in data visualization and manipulation for themselves.\nPlain-Language Statistics #There\u0026rsquo;s a lot of power in simplifying statistical language to meet an audience at their level of understanding. I love watching faculty reach an \u0026ldquo;aha\u0026rdquo;-moment with their own information through Q\u0026amp;A discussions. Similarly, teaching young students the fundamentals of object-oriented programming has given me the opportunity to assess how well I know the fundamentals myself. With both parties, I\u0026rsquo;m learning as much from them as they are from me.\nOut of Office # Boston Basin, North Cascades. Summer 2019 I\u0026rsquo;ve been lucky enough in the past several years be able to dedicate my time out of the office to exploring many of our public lands. My profile photo was taken on the rim of a cinder cone volcano in Lassen National Park. The photo above is of one of my favorite spots on the west coast.\nIf I\u0026rsquo;m not in the state, I\u0026rsquo;m likely out exploring what the rest of the West has to offer. I\u0026rsquo;m looking forward to more days out of the house, out of the office, and in the backcountry.\n","date":null,"permalink":"/about/","section":"jeslacourse","summary":"If quarantine brought me one gift, my once three-hour commute became three hours each day to build a portfolio and digress on some of the more interesting topics in statistics.","title":"About the Author"},{"content":"","date":null,"permalink":"/tags/coursework/","section":"Tags","summary":"","title":"Coursework"},{"content":"","date":null,"permalink":"/tags/datasets/","section":"Tags","summary":"","title":"Datasets"},{"content":"","date":null,"permalink":"/categories/datasets/","section":"Categories","summary":"","title":"Datasets"},{"content":"Datasets\nBirth Data - Forked from /538/data/\nhttps://github.com/jeslacourse/data/blob/master/births/US_births_2000-2014_SSA.csv\n","date":null,"permalink":"/datasets/","section":"jeslacourse","summary":"Datasets","title":"Datasets"},{"content":"\rWorking with community-oriented and socially conscious organizations to promote social awareness through meaningful data interpretation and visualization\nInstitutional Research Highlights\nTODO: Let\u0026rsquo;s share some projects and templates! Data Management and Statistical Highlights\nCreation and implementation of voter outreach GIS database for California\u0026rsquo;s 20,000 precincts. Adopted in part by major 2016 Presidential candidate campaign\nPrimary role in data management/conversion project of approximately 500,000 entries\nEarned commendations and national recognition from top management within firm\nCustom built an intuitive, dynamic vehicle maintenance workbook for 12 vehicle fleet with accompanying Fleet Procedures Manual\nCommunity Service and Leadership Highlights\nExtensive and diverse 15+ years community service track record with emphasis on youth and voter advocacy within Placer County\nServed as interim administrative supervisor and team lead through peak tax season during merger with nation\u0026rsquo;s 8th largest financial firm\nDeveloped plan of action, provided direct communication with office and regional management on immediate hiring needs\nDirect involvement in hiring and training to build a highly adept administrative team\nTeam commendation from Managing Principal for multiple seamless work seasons, citing promotion of open, positive intraoffice communication and unparalleled efficiency\nPROFESSIONAL EXPERIENCE #Senior Research Analyst (Jul 2021 - ) #Research Analyst (Jul 2020 - Jul 2021 ) #R-based Institutional Research\nCode Coach (Instructor) (Dec 2019 - Jan 2022) #Develop personalized, project-based curriculum for individuals and small groups of students aged 7-16\nPrimary Languages: Python, C++, C#\nAdapted coursework to be taught remotely in private and/or small group sessions\nSenior Administrator (Jan 2015 – Sept 2018) #Accounts Payable Clerk II (Sep 2012 – Jan 2014) #Census Follow-Up Enumerator/Interviewer (April – July 2010) #VOLUNTEER WORK #Voter Advocate 2015 - 2016, 2019 #Northern California GIS mapping support for Presidential primary candidate\nDistrict 4 (Sacramento through Central Sierras) and District 13 (Oakland) support [CA CD4 \u0026amp; CD16; 2016]\nOrganization of teams for door-to-door community interaction to promote voter registration and candidate awareness\nEmphasis on voter rights information, particularly no party preference voting rights during CA semi-open primaries\nWashoe County, NV Caucus observer and voting advocate\nCommunity Advocate, Loomis, CA 2014 - 2018 #Direct outreach to approximately 500 community members through local organization and public comments during Town Hall meetings\nEmphasis: Oil train disaster preparedness and housing development impacts on existing infrastructure\nLoomis General Plan annotation and distribution, working to promote awareness of local issues and current events to promote Town Council attendance and interaction with community leaders\nYouth Advocate, Garden Valley, CA Dec 2007 - 2009 #Head of interview and surveying committee, film editing and organization\nAttended meetings with members and partners of the non-profit film organization Tower of Youth\nEDUCATION #University of California, Davis, CA #B.S. Statistics - Data Science Track\nComputer Science Minor, Out in STEM Club Member\nRelevant Coursework\nSierra College, Rocklin, CA #A.S. Business Administration | A.S Accounting | A.S. Natural Sciences\nGeographic Information Systems Certificate\nPROGRAMMING \u0026amp; SOFTWARE EXPERIENCE #Programming Comprehension #R with RMarkdown (2018-)\nSQL, including SQL Server and PostgreSQL (2015-)\nOOP programming comprehension (2014-)\nPython (2016-) C++, Java, Javascript (1 year) HTML \u0026amp; CSS3 (2015-)\nHugo for Blogdown (2022 -) Jekyll in Ruby for Web Development (2019-2022) Specialized Software and Skills #Type Speed: 55 AWP and 10 key by touch\nFormatting: Markdown, HTML/CSS3\nRStudio (Posit) for Analysis\nArcGIS Desktop 10.1+ and Online: Mapping, spatial analytics, and data visualization resource\nJupyter Notebooks (Anaconda Suite)\nMicrosoft Office Suite: 10+ years professional experience working with Excel, Word, Outlook, PowerPoint and Access\nAccess: Proficient; join \u0026amp; relationship competency, table building, queries, data integrity\nExcel: High proficiency; building and customizing workbooks, vlookup \u0026amp; filter tables\nWord: High proficiency; business report writing, template customization, automation\n","date":null,"permalink":"/experience/","section":"jeslacourse","summary":"Working with community-oriented and socially conscious organizations to promote social awareness through meaningful data interpretation and visualization","title":"Experience"},{"content":" Show code\r# calculating user coordinates x \u0026lt;- rnorm(100) + 2 y \u0026lt;- 2 * x + 1.5 * rnorm(x) + 2 plot(jitter(x), y, pch = 20, xlab = \u0026#34;Pages\u0026#34;, ylab = \u0026#34;Awesomeness\u0026#34;, main = \u0026#34;You Are Here\u0026#34;) Oops, let\u0026rsquo;s get you back on track \u0026mdash; Try searching by tag or category.\n","date":"1 January 0001","permalink":"/404/","section":"jeslacourse","summary":"Show code\r# calculating user coordinates x \u0026lt;- rnorm(100) + 2 y \u0026lt;- 2 * x + 1.","title":"Page Not Found"},{"content":"","date":null,"permalink":"/tags/projects/","section":"Tags","summary":"","title":"Projects"},{"content":"\rB.S. Statistics - Data Science Track\nMinor - Computer Science\nHighlights #Big Data \u0026amp; High Performance Statistical Computing #High-performance computing in high-level data analysis languages; different computational approaches and paradigms for efficient analysis of big data; interfaces to compiled languages; high-level parallel computing; MapReduce; parallel algorithms and reasoning. Python. R. SQL.\nCategorical Data #Varieties of categorical data, cross-classifications, contingency tables, tests for independence. Multidimensional tables and log-linear models, maximum likelihood estimation; tests of goodness-of-fit. Logit models, linear logistic models. Analysis of incomplete tables. Packaged computer programs, analysis of real data. R.\nProject: Byssinossis (Farmer\u0026rsquo;s Lung Disease) Determination. Link TBU (Apr 2020).\nData and Web Technologies for Data Analysis #Essentials of using relational databases and SQL. Processing data in blocks. Scraping Web pages and using Web services/APIs. Basics of text mining. Interactive data visualization with Web technologies. Computational data workflow and best practices. Statistical methods. Team lead. R. SQL.\nProject: Temporal Birth Trends (2000-2015). Link TBU (Apr 2020).\nDatabase Systems #Team development and testing of L-Store style database system. Database modeling and design (E/R model, relational model), relational algebra, query languages, file and index structures, query processing, transaction management. Python. SQL.\nProject: JellyDB - GitHub\nMachine Learning #Supervised \u0026amp; unsupervised learning, including classification, dimensionality reduction, regression \u0026amp; clustering using modern machine learning methods. Applications of machine learning in biology (oncology) and engineering. Python.\nComputational Foundation and Focus #Agent-Based Modeling #Agent-based computer simulation and analysis with emphasis on learning how to model animals, including humans, to achieve insight into social and group behavior. Referred by instructor for continued studies in mate-choice and evolutionary game-theory modeling. Java.\nProject: Effects of Similiarity and Attractiveness in Mate Selection. Link TBU (Apr 2020).\nAlgorithm Design and Analysis #Complexity of algorithms, bounds on complexity, analysis methods. Searching, sorting, pattern matching, graph algorithms. Algorithm design techniques: divide-conquer, greedy, dynamic programming. Approximation methods. NP-complete problems. Python.\nComputational Linguistics #Understanding the nature of language through computer modeling of linguistic abilities. Relationships between human cognition and computer representations of cognitive processing.\nPractice in Data Science #Principles and practice of interdisciplinary, collaborative data analysis; complete case study review and team data analysis project. R.\nStatistical Data Science #Introduction to computing for data analysis and visualization, and simulation, using a high-level language. Computational reasoning, computationally intensive statistical methods, reading tabular and non-standard data. R.\nStatistical Foundation #ANOVA #Foundational experiment design. One- and Two-way ANOVA. Random effects modeling. R.\nProject: Bird-Nest Size Relationships. Link TBU (Apr 2020).\nApplied Linear Algebra #Extensive Problem Solving. Applications of linear algebra; LU and QR matrix factorizations, eigenvalue and singular value matrix decompositions. Matlab.\nProject: Classification of Handwritten Digits. Link TBU (Apr 2020).\nMathematical Statistics #Sampling, methods of estimation, bias-variance decomposition, sampling distributions, Fisher information, confidence intervals, and some elements of hypothesis testing.\nTesting theory, tools and applications from probability theory, Linear model theory, ANOVA, goodness-of-fit.\nMultivariate Analysis #Multivariate normal distribution; Mahalanobis distance; sampling distributions of the mean vector and covariance matrix; Hotellings T2; simultaneous inference; one-way MANOVA; discriminant analysis; principal components; canonical correlation; factor analysis. Intensive use of computer analyses and real data sets. R.\nProbability Theory #Fundamental concepts of probability theory, discrete and continuous random variables, standard distributions, moments and moment-generating functions, laws of large numbers and the central limit theorem. R.\nRegression Analysis #Project: Regression Notes\nSimple and multi- linear regression, variable selection techniques, stepwise regression, analysis of covariance, influence measures. R.\nStatistical Learning #Fundamental concepts and methods in statistical learning with emphasis on supervised learning. Principles, methodologies and applications of parametric and nonparametric regression, classification, resampling and model selection techniques. Python.\nReferences: # UC Davis College of Letters and Science, Department of Statistics - Course Descriptions\nUC Davis College of Engineering, Department of Computer Science - Course Description\n2019-2020 General Catalog\n","date":null,"permalink":"/projects/","section":"jeslacourse","summary":"B.","title":"Projects"},{"content":"","date":null,"permalink":"/tags/work-experience/","section":"Tags","summary":"","title":"Work Experience"}]